{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1e6d6b9",
   "metadata": {},
   "source": [
    "# Train and evaluate a PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6debe19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4608165",
   "metadata": {},
   "source": [
    "Set the random seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66933b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c005fb",
   "metadata": {},
   "source": [
    "## Load MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a02baea",
   "metadata": {},
   "source": [
    "Load the training and test splits of MNIST, and preprocess them by flattening the tensor images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a04559",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: (255 * x.view(-1)).long())\n",
    "])\n",
    "data_train = datasets.MNIST('datasets', train=True, download=True, transform=transform)\n",
    "data_test = datasets.MNIST('datasets', train=False, download=True, transform=transform)\n",
    "num_variables = data_train[0][0].shape[0]\n",
    "height, width = 28, 28\n",
    "print(f\"Number of variables: {num_variables}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad24d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(data_train[0][0].reshape(28, 28), cmap='gray')\n",
    "plt.title(f\"Class: {data_train[0][1]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965d0122",
   "metadata": {},
   "source": [
    "## Instantiating a Circuit structure Template: the Region Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d471b6e",
   "metadata": {},
   "source": [
    "Initialize a _Quad Graph_ region graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a7b261",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirkit.templates.region_graph import QuadTree\n",
    "region_graph = QuadTree(shape=(height, height))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc35ef6",
   "metadata": {},
   "source": [
    "Others available region graphs are the _Random Binary Tree_ and the _Poon Domingos_, whose imports are showed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3a1ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirkit.templates.region_graph import RandomBinaryTree, PoonDomingos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66900897-5d65-4136-8399-f997c3665a38",
   "metadata": {},
   "source": [
    "## Constructing the Symbolic Circuit Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6468f6-a1e5-46f8-92d4-d1eb6fa5ba19",
   "metadata": {},
   "source": [
    "From the region graph definition above, we now construct the symbolic circuit representation. Note that this circuit representation is _not_ executable, i.e., you cannot do learn it or do inference with it. It will be compiled later, by choosing a backend such as torch.\n",
    "\n",
    "To do so, we first define the factories that will be used to construct symbolic layers. Note that we choose the parameterization at the symbolic level. That is, we guarantee non-negative parameters by passing them through an exponential function. Moreover, we can choose how to parameterize the categorical distributions used to model the distribution of pixel values in the 0-255 range. In this case, we use a log softmax function. We choose to initialize the weights of the circuit by sampling from a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0a1895-999f-47af-8c6f-57242726540a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirkit.utils.scope import Scope\n",
    "from cirkit.symbolic.parameters import LogSoftmaxParameter, ExpParameter, Parameter\n",
    "from cirkit.symbolic.layers import CategoricalLayer, DenseLayer, KroneckerLayer, MixingLayer\n",
    "from cirkit.symbolic.initializers import NormalInitializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96b9dfc-83a6-43ba-b20d-48d6dd79e663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_layer_factory(\n",
    "    scope: Scope,\n",
    "    num_units: int,\n",
    "    num_channels: int\n",
    ") -> CategoricalLayer:\n",
    "    return CategoricalLayer(\n",
    "        scope, num_units, num_channels, num_categories=256,\n",
    "        parameterization=lambda p: Parameter.from_unary(LogSoftmaxParameter(p.shape), p),\n",
    "        initializer=NormalInitializer(0.0, 1e-2)\n",
    "    )\n",
    "\n",
    "def kronecker_layer_factory(\n",
    "    scope: Scope, num_input_units: int, arity: int\n",
    ") -> HadamardLayer:\n",
    "    return KroneckerLayer(scope, num_input_units, arity)\n",
    "\n",
    "def dense_layer_factory(\n",
    "    scope: Scope,\n",
    "    num_input_units: int,\n",
    "    num_output_units: int\n",
    ") -> DenseLayer:\n",
    "    return DenseLayer(\n",
    "        scope, num_input_units, num_output_units,\n",
    "        parameterization=lambda p: Parameter.from_unary(ExpParameter(p.shape), p),\n",
    "        initializer=NormalInitializer(0.0, 1e-1)\n",
    "    )\n",
    "\n",
    "\n",
    "def mixing_layer_factory(\n",
    "    scope: Scope, num_units: int, arity: int\n",
    ") -> MixingLayer:\n",
    "    return MixingLayer(\n",
    "        scope, num_units, arity,\n",
    "        parameterization=lambda p: Parameter.from_unary(ExpParameter(p.shape), p),\n",
    "        initializer=NormalInitializer(0.0, 1e-1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9707de19-d6c7-4646-8266-2e20dcefa22e",
   "metadata": {},
   "source": [
    "Then, we call a function to construct the symbolic circuit from region graph, by specifying the number of units and the factories to build layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97be2b8f-0012-46ed-9f03-26a7a4729b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirkit.symbolic.circuit import Circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c88f38-1552-4d13-b62d-931493c07c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbolic_circuit = Circuit.from_region_graph(\n",
    "    region_graph,\n",
    "    num_input_units=8,\n",
    "    num_sum_units=8,\n",
    "    input_factory=categorical_layer_factory,\n",
    "    sum_factory=dense_layer_factory,\n",
    "    prod_factory=kronecker_layer_factory,\n",
    "    mixing_factory=mixing_layer_factory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8c6e7c-ad9f-4dd2-ab76-602e191d197b",
   "metadata": {},
   "source": [
    "We can retrieve some information about the circuit and its structural properties as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23570bfd-a64e-4e19-ba4c-30e489e9d08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Smooth: {symbolic_circuit.is_smooth}')\n",
    "print(f'Decomposable: {symbolic_circuit.is_decomposable}')\n",
    "print(f'Number of variables: {symbolic_circuit.num_variables}')\n",
    "print(f'Number of channels per variable: {symbolic_circuit.num_channels}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67398c38",
   "metadata": {},
   "source": [
    "## Compiling the Symbolic Circuit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba547d8",
   "metadata": {},
   "source": [
    "We are ready to compile the symbolic circuit constructed above into another one that we can learn and/or do inference. To do so, we have to choose a compilation backend. In this case, we choose torch as a backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ea4a4a-649d-462f-bcfd-20a12bd8a052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda')  # The device to use\n",
    "torch.manual_seed(42)\n",
    "if 'cuda' in device.type:\n",
    "    torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f7a3f9-1c64-4117-81f0-8766ccbd3179",
   "metadata": {},
   "source": [
    "We first need to instantiate a circuit pipeline context and specify the backend to be used, as well as optional compilation flags, e.g., whether to fold the circuit or which inference semiring to use. Finally, we use the pipeline context to compile the symbolic circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af58c11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirkit.pipeline import PipelineContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a1892e-4a65-4759-bb3a-ccbe6f5e515c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ctx = PipelineContext(\n",
    "    backend='torch',   # Choose the torch compilation backend\n",
    "    fold=True,         # Fold the circuit, this is a backend-specific compilation flag\n",
    "    semiring='lse-sum' # Use the (R, +, *) semiring, where + is the log-sum-exp and * is the sum\n",
    ")\n",
    "circuit = ctx.compile(symbolic_circuit)\n",
    "\n",
    "# Alternatively, one can use the Python _with_ statement to compile circuits inside of it.\n",
    "#\n",
    "# from cirkit.pipeline import compile\n",
    "# with PipelineContext(backend='torch', fold=True, semiring='lse-sum') as ctx:\n",
    "#    circuit = compile(symbolic_circuit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd14baf-a29e-4773-84a7-417d8747f678",
   "metadata": {},
   "source": [
    "Note that the compilation step, comprising the folding optimization, required about 1 second for a circuit with almost 5000 layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f0d55e-05c3-42ac-a63c-326f8446f90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(list(symbolic_circuit.layers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ce78e8-79fc-4630-8cea-ce6e0df4429b",
   "metadata": {},
   "source": [
    "We observe how the tensorized circuit has much fewer layers, as the great majority has been folded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7965af-05d3-472f-84cb-7c3d80da52fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(circuit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beee1f04",
   "metadata": {},
   "source": [
    "## Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bf4858",
   "metadata": {},
   "source": [
    "We are now ready to learn the parameters and do inference First, we wrap our data into PyTorch data loaders by specifying the batch size. Then, we initialize any PyTorch optimizer, e.g. SGD with momentum in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02854883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(data_train, shuffle=True, batch_size=256, drop_last=True, num_workers=4)\n",
    "test_dataloader = DataLoader(data_test, shuffle=False, batch_size=256, num_workers=4)\n",
    "optimizer = optim.SGD(circuit.parameters(), lr=0.1, momentum=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab362742-03af-4839-a302-44c8492a370e",
   "metadata": {},
   "source": [
    "### Compiling the Circuit computing the Partition Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbac573d",
   "metadata": {},
   "source": [
    "Here, we choose to optimize the parameters by minimizing the negative log-likelihood. However, since the circuit is not already normalized (as we parameterized the sums using an exponential), we need to instantiate a circuit computing the partition function explicitly. That is, we integrate the circuit within the pipeline context as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1ae885-d02a-4c94-829e-dc491a8c4a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_circuit = ctx.integrate(circuit)\n",
    "\n",
    "# Alternatively, one can use the _with_ statement.\n",
    "#\n",
    "# from cirkit.pipeline import integrate\n",
    "# with ctx:\n",
    "#     pf_circuit = integrate(tensorized_circuit)\n",
    "\n",
    "# Another way to perform the integration is by using the _symbolic functional_ APIs.\n",
    "# This gives us additional flexibility by operating over circuits _before compiling them_.\n",
    "#\n",
    "# import cirkit.symbolic.functional as SF\n",
    "# with ctx:\n",
    "#     pf_symbolic_circuit = SF.integrate(symbolic_circuit)\n",
    "#     pf_circuit = compile(pf_symbolic_circuit)\n",
    "\n",
    "# The type of the circuit obtained by integration of all variables is a 'constant' circuit,\n",
    "# i.e., it does not take inputs.\n",
    "print(type(pf_circuit))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e05e268-4ab0-4c65-94b6-f624329bee55",
   "metadata": {},
   "source": [
    "Note that the context will take care of the parameters being shared between the compiled circuit and its other version, which computes the partition function. Finally, we learn the parameters by minimizing the negative log-likelihood, similarly as any other model in torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb6683d-f7c0-4b70-afd1-912a90861c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move circuits to device\n",
    "circuit = circuit.to(device)\n",
    "pf_circuit = pf_circuit.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e025438-441d-4965-8deb-12178f0e6d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f28e9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.perf_counter()\n",
    "num_epochs = 3\n",
    "step_idx = 0\n",
    "running_loss = 0.0\n",
    "for epoch_idx in range(num_epochs):\n",
    "    for i, (batch, _) in enumerate(train_dataloader):\n",
    "        batch = batch.to(device).unsqueeze(dim=1)   # Add a channel dimension\n",
    "        log_output = circuit(batch)                 # Compute the log output of the circuit\n",
    "        log_pf = pf_circuit()                       # Compute the log partition function of the circuit\n",
    "        lls = log_output - log_pf                   # Compute the log-likelihood\n",
    "        loss = -torch.mean(lls)   # The loss is the negative average log-likelihood\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        running_loss += loss.detach() * len(batch)\n",
    "        step_idx += 1\n",
    "        if step_idx % 100 == 0:\n",
    "            print(f\"Step {step_idx}: Average NLL: {running_loss / (100 * len(batch)):.3f}\")\n",
    "            running_loss = 0.0\n",
    "end_time = time.perf_counter()\n",
    "print(f\"Training time: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50efae6b",
   "metadata": {},
   "source": [
    "We then evaluate our model on test data by computing the average log-likelihood and bits per dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e66bd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "circuit.eval()\n",
    "pf_circuit.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_lls = 0.0\n",
    "    log_pf = pf_circuit()  # Compute the log partition function of the circuit (just once as we are evaluating)\n",
    "    for batch, _ in test_dataloader:\n",
    "        batch = batch.to(device).unsqueeze(dim=1)   # Add a channel dimension\n",
    "        log_output = circuit(batch)                 # Compute the log output of the circuit\n",
    "        lls = log_output - log_pf                   # Compute the log-likelihood\n",
    "        test_lls += lls.sum().item()\n",
    "    average_ll = test_lls / len(data_test)\n",
    "    bpd = -average_ll / (num_variables * np.log(2.0))\n",
    "    print(f\"Average test LL: {average_ll:.3f}\")\n",
    "    print(f\"Bits per dimension: {bpd}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b59062-a699-4785-a532-f00b03c55d13",
   "metadata": {},
   "source": [
    "# Bonus: Optimize your Circuit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b018dd37-7be3-4e90-b912-77a68e873279",
   "metadata": {},
   "source": [
    "The circuit that we learned above contains expensive kronecker product layers that can be optimized by fusing them with the dense layers.\n",
    "In the following, we compile the circuit by activating the optimize flag of the compiler.\n",
    "Based on the layers being used, this will often yield a different torch circuit that is more time and memory efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0b558b-dc64-4dd8-ab4b-f898a92110f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ctx = PipelineContext(\n",
    "    backend='torch',   # Choose the torch compilation backend\n",
    "    fold=True,         # Fold the circuit, this is a backend-specific compilation flag\n",
    "    optimize=True,     # Active the optimizations of the torch compilatio backend\n",
    "    semiring='lse-sum' # Use the (R, +, *) semiring, where + is the log-sum-exp and * is the sum\n",
    ")\n",
    "circuit = ctx.compile(symbolic_circuit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03fc5a4-3a3d-4a6a-9be4-7eaf86d4b499",
   "metadata": {},
   "source": [
    "Note that the optimized circuit will now have the Tucker layer, which is a fused layer combining the kronecker and dense layers that is much more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d171797c-a275-4166-b122-e658de04358e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(circuit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e5f7cd-9438-45e4-b14f-c1467f78c63f",
   "metadata": {},
   "source": [
    "We learn again the parameters of the circuit using the same code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377f5c23-ea27-496f-a0a9-72b1905e56c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(data_train, shuffle=True, batch_size=256, drop_last=True, num_workers=4)\n",
    "test_dataloader = DataLoader(data_test, shuffle=False, batch_size=256, num_workers=4)\n",
    "optimizer = optim.SGD(circuit.parameters(), lr=0.1, momentum=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d840f71-0a4d-4ee9-9414-8b4ed26d90b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c596bfe-e4c3-41b5-9428-b70616e1595b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.perf_counter()\n",
    "num_epochs = 3\n",
    "step_idx = 0\n",
    "running_loss = 0.0\n",
    "for epoch_idx in range(num_epochs):\n",
    "    for i, (batch, _) in enumerate(train_dataloader):\n",
    "        batch = batch.to(device).unsqueeze(dim=1)   # Add a channel dimension\n",
    "        log_output = circuit(batch)                 # Compute the log output of the circuit\n",
    "        log_pf = pf_circuit()                       # Compute the log partition function of the circuit\n",
    "        lls = log_output - log_pf                   # Compute the log-likelihood\n",
    "        loss = -torch.mean(lls)   # The loss is the negative average log-likelihood\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        running_loss += loss.detach() * len(batch)\n",
    "        step_idx += 1\n",
    "        if step_idx % 100 == 0:\n",
    "            print(f\"Step {step_idx}: Average NLL: {running_loss / (100 * len(batch)):.3f}\")\n",
    "            running_loss = 0.0\n",
    "end_time = time.perf_counter()\n",
    "print(f\"Training time: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550a89d8-26a9-4ccd-afac-23dd15e4e057",
   "metadata": {},
   "source": [
    "We observe a reduced training time given by the optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79e35d1-b4bb-4741-8e32-b5e41fa56c28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
