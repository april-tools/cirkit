{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3808c90-c7a4-4365-bf3d-910d8f707122",
   "metadata": {},
   "source": [
    "# Conditional Circuits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f37603a-6155-4792-9dee-4e837d9c8d0d",
   "metadata": {},
   "source": [
    "Let's assume we want to parameterize a circuit by means of a neural network, i.e., build and learn a _conditional circuit_. We can do so in cirkit in three steps:\n",
    "1. we instantiate the symbolic circuit we want to parameterize;\n",
    "2. we call a functional that takes the symbolic circuit and returns another one that contains the additional information for the parameterization we want;\n",
    "3. we compile the symbolic circuit by firstly registering the parameterization to the compiler.\n",
    "\n",
    "We start by instantiating a symbolic circuit on MNSIT images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "788ab2da-06ed-4b1b-ab5e-5f3ea0b48644",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirkit.templates import data_modalities, utils\n",
    "\n",
    "symbolic_circuit = data_modalities.image_data(\n",
    "    (1, 28, 28),                 # The shape of MNIST image, i.e., (num_channels, image_height, image_width)\n",
    "    region_graph='quad-tree-4',  # Select the structure of the circuit to follow the QuadTree-4 region graph\n",
    "    input_layer='categorical',   # Use Categorical distributions for the pixel values (0-255) as input layers\n",
    "    num_input_units=64,          # Each input layer consists of 64 Categorical input units\n",
    "    sum_product_layer='cp',      # Use CP sum-product layers, i.e., alternate dense layers with Hadamard product layers\n",
    "    num_sum_units=64,            # Each dense sum layer consists of 64 sum units\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01a42db-d188-4bfd-9697-bcc8d88f81b8",
   "metadata": {},
   "source": [
    "Note that we did not specify any parameterization for the sum layer parameters and the logits of the Categorical input layers.\n",
    "\n",
    "Then, we call the functional ```cirkit.symbolic.functional.model_parameterize``` to obtain another symbolic circuit that stores the additional information on how we want to parameterize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa0241f1-ae5a-4ac2-aef8-0de0a4e68d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cirkit.symbolic.functional as SF\n",
    "from cirkit.symbolic.layers import CategoricalLayer, SumLayer\n",
    "\n",
    "parametrization_map = {\n",
    "    \"sum-layers\": list(symbolic_circuit.sum_layers)\n",
    "}\n",
    "\n",
    "# Assume there exists a model called \"my-neural-network\" we will specify at compile-time later\n",
    "symbolic_conditional_circuit, model_specs = SF.model_parameterize(\n",
    "    symbolic_circuit,                          # The symbolic circut we want to parameterize\n",
    "    gate_functions=parametrization_map\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8450697-f455-477d-bb70-9f5206e02206",
   "metadata": {},
   "source": [
    "The ```model_parameterize``` functional also returns the shapes of the tensors to parameterize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "563ec2fa-9892-4a02-868d-1bac56e11d2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'g0.SumLayer.weight': (1048, 64, 64), 'g1.SumLayer.weight': (1, 1, 64)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The parameterize function returned the shape specfication of the tensors we will need to return\n",
    "model_specs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b00231-b1d2-4eba-9ca1-21c315093293",
   "metadata": {},
   "source": [
    "Before compiling our conditional circuit, we define a neural network in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29476ba5-3337-43f0-af55-1792e6e51526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor, nn\n",
    "\n",
    "class MyNeuralNetwork(nn.Module):\n",
    "    def __init__(self, output_specs: dict[str, tuple[int, ...]]):\n",
    "        super().__init__()\n",
    "        self._output_specs = output_specs\n",
    "\n",
    "    def forward(self, z: Tensor) -> dict[str, Tensor]:\n",
    "        # Evaluate my neural network on some input y\n",
    "        # Return a dictionary mapping parameter names to actual parameter tensors\n",
    "        # In this example, we just sample them randomly and use a softmax activation\n",
    "        mean, stddev = torch.mean(z.flatten()), torch.std(z.flatten())\n",
    "        return {\n",
    "            name: torch.softmax(mean + stddev * torch.randn(*shape), dim=-1)\n",
    "            for name, shape in self._output_specs.items()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85065c06-10e5-41a9-9a4e-ead9f9e04405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate our neural network\n",
    "my_neural_net = MyNeuralNetwork(model_specs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dddcc21-9bf8-4d88-a42a-59577b3dd4d2",
   "metadata": {},
   "source": [
    "Next, we tell the compiler about our model, by specifying the same model name we gave to the ```model_parameterize``` functional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb80d8a0-2ddf-40fd-a837-91ad7171406d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirkit.pipeline import PipelineContext\n",
    "\n",
    "# Initialize an pipeline compilation context\n",
    "# Let's try _without_ folding first\n",
    "ctx = PipelineContext(semiring=\"lse-sum\", backend='torch', fold=False, optimize=False)\n",
    "\n",
    "# Register our neural network as an external model\n",
    "ctx.add_gate_function(\"sum-layers\", my_neural_net)\n",
    "\n",
    "# Finally, we compile the conditional circuit\n",
    "circuit = ctx.compile(symbolic_conditional_circuit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1dd09e-ca5b-41e4-bf3c-365c747a1926",
   "metadata": {},
   "source": [
    "Now, to evaluate the conditional circuit we specify additional arguments when calling it on some tensor, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "021de16a-44aa-4da9-ac61-18f04ddfe835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-4356.2993]],\n",
       "\n",
       "        [[-4353.8691]],\n",
       "\n",
       "        [[-4364.9561]],\n",
       "\n",
       "        [[-4356.7891]],\n",
       "\n",
       "        [[-4359.1729]],\n",
       "\n",
       "        [[-4361.8467]],\n",
       "\n",
       "        [[-4355.8101]],\n",
       "\n",
       "        [[-4358.1465]]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 8\n",
    "x = torch.randint(256, size=(batch_size, 1, 784))  # The circuit input\n",
    "# TODO: make the parameters batch-dependant\n",
    "z = torch.randn(size=(1, 127))  # Some dummy input to the neural net\n",
    "\n",
    "# Evaluate the circuit on some input\n",
    "# Note that we also pass some input to the external model\n",
    "circuit(x, gate_function_kwargs={'sum-layers': {'z': z}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9d2ac4-0b13-4d44-9634-cdb8e2d58cf1",
   "metadata": {},
   "source": [
    "The above parameterization is robust to change in compilation flages, e.g., now enabling folding and layer optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0173f15-e819-496d-a376-97d6b765cb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirkit.pipeline import PipelineContext\n",
    "\n",
    "# Initialize an pipeline compilation context\n",
    "# Let's try _without_ folding first\n",
    "ctx = PipelineContext(semiring=\"lse-sum\", backend='torch', fold=True, optimize=True)\n",
    "\n",
    "# Register our neural network as an external model\n",
    "ctx.add_gate_function(\"sum-layers\", my_neural_net)\n",
    "\n",
    "# Finally, we compile the conditional circuit\n",
    "circuit = ctx.compile(symbolic_conditional_circuit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a165dab0-40b6-4981-97af-86506f4f07cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-4356.0820]],\n",
       "\n",
       "        [[-4351.3091]],\n",
       "\n",
       "        [[-4356.7505]],\n",
       "\n",
       "        [[-4362.7915]],\n",
       "\n",
       "        [[-4359.5459]],\n",
       "\n",
       "        [[-4352.6587]],\n",
       "\n",
       "        [[-4363.6162]],\n",
       "\n",
       "        [[-4354.9424]]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 8\n",
    "x = torch.randint(256, size=(batch_size, 1, 784))  # The circuit input\n",
    "# TODO: make the parameters batch-dependant\n",
    "z = torch.randn(size=(1, 127))  # Some dummy input to the neural net\n",
    "\n",
    "# Evaluate the circuit on some input\n",
    "# Note that we also pass some input to the external model\n",
    "circuit(x, gate_function_kwargs={'sum-layers': {'z': z}})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cirkit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
