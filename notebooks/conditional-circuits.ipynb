{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3808c90-c7a4-4365-bf3d-910d8f707122",
   "metadata": {},
   "source": [
    "# Conditional Circuits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f37603a-6155-4792-9dee-4e837d9c8d0d",
   "metadata": {},
   "source": [
    "Let's assume we want to parameterize a circuit by means of a neural network, i.e., build and learn a _conditional circuit_. We can do so in cirkit in three steps:\n",
    "1. we instantiate the symbolic circuit we want to parameterize;\n",
    "2. we call a functional that takes the symbolic circuit and returns another one that contains the additional information for the parameterization we want;\n",
    "3. we compile the symbolic circuit by firstly registering the parameterization to the compiler.\n",
    "\n",
    "We start by instantiating a symbolic circuit on MNSIT images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dff036b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "788ab2da-06ed-4b1b-ab5e-5f3ea0b48644",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirkit.templates import data_modalities, utils\n",
    "\n",
    "symbolic_circuit = data_modalities.image_data(\n",
    "    (1, 28, 28),                 # The shape of MNIST image, i.e., (num_channels, image_height, image_width)\n",
    "    region_graph='quad-tree-4',  # Select the structure of the circuit to follow the QuadTree-4 region graph\n",
    "    input_layer='categorical',   # Use Categorical distributions for the pixel values (0-255) as input layers\n",
    "    num_input_units=64,          # Each input layer consists of 64 Categorical input units\n",
    "    sum_product_layer='cp',      # Use CP sum-product layers, i.e., alternate dense layers with Hadamard product layers\n",
    "    num_sum_units=64,            # Each dense sum layer consists of 64 sum units\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01a42db-d188-4bfd-9697-bcc8d88f81b8",
   "metadata": {},
   "source": [
    "Note that we did not specify any parameterization for the sum layer parameters and the logits of the Categorical input layers.\n",
    "\n",
    "Then, we call the functional ```cirkit.symbolic.functional.model_parameterize``` to obtain another symbolic circuit that stores the additional information on how we want to parameterize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa0241f1-ae5a-4ac2-aef8-0de0a4e68d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cirkit.symbolic.functional as SF\n",
    "from cirkit.symbolic.layers import CategoricalLayer, SumLayer\n",
    "\n",
    "parametrization_map = {\n",
    "    \"sum-layers\": list(symbolic_circuit.sum_layers)\n",
    "}\n",
    "\n",
    "# Assume there exists a model called \"my-neural-network\" we will specify at compile-time later\n",
    "symbolic_conditional_circuit = SF.condition_circuit(\n",
    "    symbolic_circuit,                          # The symbolic circut we want to parameterize\n",
    "    gate_functions=parametrization_map\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8450697-f455-477d-bb70-9f5206e02206",
   "metadata": {},
   "source": [
    "The ```model_parameterize``` functional also returns the shapes of the tensors to parameterize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "563ec2fa-9892-4a02-868d-1bac56e11d2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sum-layers.weight.0': (1048, 64, 64), 'sum-layers.weight.1': (1, 1, 64)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The parameterize function returned the shape specfication of the tensors we will need to return\n",
    "gf_specs = symbolic_conditional_circuit.gate_function_specs\n",
    "gf_specs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b00231-b1d2-4eba-9ca1-21c315093293",
   "metadata": {},
   "source": [
    "Before compiling our conditional circuit, we define the gating function. As long as the gating function outputs a tensor compatible with the shape specified by the gating functions specifications, they can be any arbitrary function.\n",
    "\n",
    "Note that the function is responsible for providing valid parameters For instance, we have to make sure that for each sum parameter the sum of its weights is $1$.\n",
    "\n",
    "Let's first parametrize the sum layers of the circuit by randomly sampling their weights and normalizing using a softmax activation. To do so, we define gating functions that take as input an external tensor, say `z`, and outputs tensors with shapes compatible with the specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73c7243f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights shape: torch.Size([3, 1048, 64, 64])\n",
      "Weight are normalized: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from functools import partial\n",
    "\n",
    "def random_sum_weights(shape, z: torch.Tensor):\n",
    "    # compute the mean and standard deviation of all the elements in the batch\n",
    "    mean, stddev = torch.mean(z, dim=-1), torch.std(z, dim=-1)\n",
    "    # compute weights by randomly sampling\n",
    "    samples = torch.randn(*shape)\n",
    "    weight = mean.view(-1, 1, 1, 1) + stddev.view(-1, 1, 1, 1) * samples\n",
    "    # normalize weights using softmax\n",
    "    return torch.softmax(weight, dim=-1)\n",
    "\n",
    "# test that the function outputs proper weights\n",
    "weights = random_sum_weights(\n",
    "    (3, *symbolic_conditional_circuit.gate_function_specs[\"sum-layers.weight.0\"]), \n",
    "    torch.randn(3, 256)\n",
    ")\n",
    "\n",
    "print(\"Weights shape:\", weights.shape)\n",
    "print(\"Weight are normalized:\", weights.sum(dim=-1).allclose(torch.tensor([1.0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e166702",
   "metadata": {},
   "source": [
    "We can now register the gating functions on the compiler, which will take care of compiling the conditional circuit, keep track of which function to call and execute them efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2635a086",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirkit.pipeline import PipelineContext\n",
    "\n",
    "# Initialize an pipeline compilation context\n",
    "# Let's try _without_ folding first\n",
    "ctx = PipelineContext(semiring=\"lse-sum\", backend='torch', fold=False, optimize=False)\n",
    "\n",
    "# Register our neural network as an external model\n",
    "ctx.add_gate_function(\"sum-layers.weight.0\", random_sum_weights)\n",
    "ctx.add_gate_function(\"sum-layers.weight.1\", random_sum_weights)\n",
    "\n",
    "# Finally, we compile the conditional circuit\n",
    "circuit = ctx.compile(symbolic_conditional_circuit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456732e3",
   "metadata": {},
   "source": [
    "And evaluate the conditional circuit by specifying the argument for each gating function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddb66bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-4357.7739]],\n",
       "\n",
       "        [[-4355.7095]],\n",
       "\n",
       "        [[-4356.8057]],\n",
       "\n",
       "        [[-4352.9370]],\n",
       "\n",
       "        [[-4356.2651]],\n",
       "\n",
       "        [[-4353.3125]],\n",
       "\n",
       "        [[-4371.3247]],\n",
       "\n",
       "        [[-4357.3862]],\n",
       "\n",
       "        [[-4361.4160]],\n",
       "\n",
       "        [[-4363.3970]]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randint(256, size=(10, 784))  # The circuit input\n",
    "z = torch.randn(size=(10, 127))  # Some dummy input to the neural net\n",
    "\n",
    "# Evaluate the circuit on some input\n",
    "# Note that we also pass some input to the external model\n",
    "circuit(x, gate_function_kwargs={'sum-layers.weight.0': {'z': z}, 'sum-layers.weight.1': {'z': z}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9d2ac4-0b13-4d44-9634-cdb8e2d58cf1",
   "metadata": {},
   "source": [
    "The above parameterization is robust to change in compilation flages, e.g., now enabling folding and layer optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0173f15-e819-496d-a376-97d6b765cb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folding and optimization is enabled\n",
    "ctx = PipelineContext(semiring=\"lse-sum\", backend='torch', fold=True, optimize=True)\n",
    "\n",
    "ctx.add_gate_function(\"sum-layers.weight.0\", random_sum_weights)\n",
    "ctx.add_gate_function(\"sum-layers.weight.1\", random_sum_weights)\n",
    "\n",
    "circuit = ctx.compile(symbolic_conditional_circuit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58740287",
   "metadata": {},
   "source": [
    "And evaluate it just like a regular conditional circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a165dab0-40b6-4981-97af-86506f4f07cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-4361.3394]],\n",
       "\n",
       "        [[-4360.3687]],\n",
       "\n",
       "        [[-4360.0630]],\n",
       "\n",
       "        [[-4359.3193]],\n",
       "\n",
       "        [[-4358.3257]],\n",
       "\n",
       "        [[-4350.2729]],\n",
       "\n",
       "        [[-4351.1704]],\n",
       "\n",
       "        [[-4351.4927]],\n",
       "\n",
       "        [[-4362.0557]],\n",
       "\n",
       "        [[-4356.7437]]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "circuit(x, gate_function_kwargs={'sum-layers.weight.0': {'z': z}, 'sum-layers.weight.1': {'z': z}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38970c95",
   "metadata": {},
   "source": [
    "The conditional parametrization is batch-dependant: for each batch we independently parametrize the model. To see this, let's change the gate function for sums such that it parametrizes all sum layer with all zero weigths only for the first element in the batch. Intuitively, we should see the circuit producing a *stange* likelihood on the first batch and working regularly on the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "187eff40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[      -inf]],\n",
       "\n",
       "        [[-4359.3535]],\n",
       "\n",
       "        [[-4359.1943]],\n",
       "\n",
       "        [[-4360.5337]],\n",
       "\n",
       "        [[-4359.6562]],\n",
       "\n",
       "        [[-4358.2544]],\n",
       "\n",
       "        [[-4357.5913]],\n",
       "\n",
       "        [[-4352.2979]],\n",
       "\n",
       "        [[-4360.2314]],\n",
       "\n",
       "        [[-4361.7720]]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def random_sum_weights_zero_first_sample(shape, z: torch.Tensor):\n",
    "    # compute the mean and standard deviation of all the elements in the batch\n",
    "    mean, stddev = torch.mean(z, dim=-1), torch.std(z, dim=-1)\n",
    "    # compute weights by randomly sampling\n",
    "    samples = torch.randn(*shape)\n",
    "    weight = mean.view(-1, 1, 1, 1) + stddev.view(-1, 1, 1, 1) * samples\n",
    "    # normalize weights using softmax\n",
    "    weight = torch.softmax(weight, dim=-1)\n",
    "    \n",
    "    # set first element in batch to 0\n",
    "    weight[0] = 0.0\n",
    "\n",
    "    return weight\n",
    "\n",
    "# register the new gate function and compile the circuit\n",
    "ctx = PipelineContext(semiring=\"lse-sum\", backend='torch', fold=True, optimize=True)\n",
    "ctx.add_gate_function(\"sum-layers.weight.0\", random_sum_weights_zero_first_sample)\n",
    "ctx.add_gate_function(\"sum-layers.weight.1\", random_sum_weights_zero_first_sample)\n",
    "circuit = ctx.compile(symbolic_conditional_circuit)\n",
    "\n",
    "# run the circuit on the same dummy inputs\n",
    "circuit(x, gate_function_kwargs={'sum-layers.weight.0': {'z': z}, 'sum-layers.weight.1': {'z': z}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9947e28",
   "metadata": {},
   "source": [
    "Indeed, the first batch evaluates to a negative log likelihood equal to $-\\infty$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cirkit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
