{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3808c90-c7a4-4365-bf3d-910d8f707122",
   "metadata": {},
   "source": [
    "# Conditional Circuits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f37603a-6155-4792-9dee-4e837d9c8d0d",
   "metadata": {},
   "source": [
    "Let's assume we want to parameterize a circuit by means of a neural network, i.e., build and learn a _conditional circuit_. We can do so in cirkit in three steps:\n",
    "1. we instantiate the symbolic circuit we want to parameterize;\n",
    "2. we call a functional that takes the symbolic circuit and returns another one that contains the additional information for the parameterization we want;\n",
    "3. we compile the symbolic circuit by firstly registering the parameterization to the compiler.\n",
    "\n",
    "We start by instantiating a symbolic circuit on MNSIT images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "788ab2da-06ed-4b1b-ab5e-5f3ea0b48644",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirkit.templates import data_modalities, utils\n",
    "\n",
    "symbolic_circuit = data_modalities.image_data(\n",
    "    (1, 28, 28),                 # The shape of MNIST image, i.e., (num_channels, image_height, image_width)\n",
    "    region_graph='quad-tree-4',  # Select the structure of the circuit to follow the QuadTree-4 region graph\n",
    "    input_layer='categorical',   # Use Categorical distributions for the pixel values (0-255) as input layers\n",
    "    num_input_units=64,          # Each input layer consists of 64 Categorical input units\n",
    "    sum_product_layer='cp',      # Use CP sum-product layers, i.e., alternate dense layers with Hadamard product layers\n",
    "    num_sum_units=64,            # Each dense sum layer consists of 64 sum units\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01a42db-d188-4bfd-9697-bcc8d88f81b8",
   "metadata": {},
   "source": [
    "Note that we did not specify any parameterization for the sum layer parameters and the logits of the Categorical input layers.\n",
    "\n",
    "Then, we call the functional ```cirkit.symbolic.functional.condition_circuit``` to obtain another symbolic circuit that stores the additional information on how we want to parameterize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa0241f1-ae5a-4ac2-aef8-0de0a4e68d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cirkit.symbolic.functional as SF\n",
    "from cirkit.symbolic.layers import CategoricalLayer, SumLayer\n",
    "\n",
    "parametrization_map = {\n",
    "    \"sum-layers\": list(symbolic_circuit.sum_layers)\n",
    "}\n",
    "\n",
    "symbolic_conditional_circuit, gf_specs = SF.condition_circuit(\n",
    "    symbolic_circuit,                          # The symbolic circut we want to parameterize\n",
    "    gate_functions=parametrization_map\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8450697-f455-477d-bb70-9f5206e02206",
   "metadata": {},
   "source": [
    "The ```condition_circuit``` functional also returns the shapes of the tensors to parameterize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "563ec2fa-9892-4a02-868d-1bac56e11d2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sum-layers.weight.0': (1048, 64, 64), 'sum-layers.weight.1': (1, 1, 64)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The parameterize function returned the shape specfication of the tensors we will need to return\n",
    "gf_specs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b00231-b1d2-4eba-9ca1-21c315093293",
   "metadata": {},
   "source": [
    "Before compiling our conditional circuit, we define the gating function. As long as the gating function outputs a tensor compatible with the shape specified by the gating functions specifications, they can be any arbitrary function.\n",
    "\n",
    "Note that the function is only responsible for providing valid parameters. Additional operations that are declared by the symbolic circuit are mantained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc534632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter graph for a sum layer in the symbolic circuit:\n",
      "\t TensorParameter(shape=(64, 64), learnable=True, dtype=2, initializer=NormalInitializer(mean=0.0, stddev=1.0))\n",
      "\t <cirkit.symbolic.parameters.SoftmaxParameter object at 0x703441fad4c0>\n"
     ]
    }
   ],
   "source": [
    "print(\"Parameter graph for a sum layer in the symbolic circuit:\")\n",
    "for n in next(symbolic_circuit.sum_layers).params[\"weight\"].nodes:\n",
    "    print(\"\\t\", n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cded1d",
   "metadata": {},
   "source": [
    "In the original symbolic circuit, we have one tensor parameterizing the sum layer weights and a softmax activation that ensures they form a convex combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37bb78d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter graph for a sum layer in the conditioned circuit:\n",
      "\t GateFunctionParameter(shape=(64, 64), name='sum-layers.weight.0', index=0)\n",
      "\t <cirkit.symbolic.parameters.SoftmaxParameter object at 0x703441fad4c0>\n"
     ]
    }
   ],
   "source": [
    "print(\"Parameter graph for a sum layer in the conditioned circuit:\")\n",
    "for n in next(symbolic_conditional_circuit.sum_layers).params[\"weight\"].nodes:\n",
    "    print(\"\\t\", n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fed11f8",
   "metadata": {},
   "source": [
    "The same happens in the conditional circuit, but with the crucial difference that the tensor parameter has been replaced by a gate function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ae38ca",
   "metadata": {},
   "source": [
    "For a simple test, let's parametrize the sum layers of the circuit by randomly sampling their weights. To do so, we define gating functions that take as input an external tensor, say `z`, and outputs tensors with shapes compatible with the specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73c7243f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights shape: torch.Size([3, 1048, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from functools import partial\n",
    "\n",
    "def random_sum_weights(shape, z: torch.Tensor):\n",
    "    # compute the mean and standard deviation of all the elements in the batch\n",
    "    mean, stddev = torch.mean(z, dim=-1), torch.std(z, dim=-1)\n",
    "    # compute weights by randomly sampling\n",
    "    samples = torch.randn(*shape)\n",
    "    weight = mean.view(-1, 1, 1, 1) + stddev.view(-1, 1, 1, 1) * samples\n",
    "    return weight\n",
    "\n",
    "# test that the function outputs proper weights\n",
    "weights = random_sum_weights(\n",
    "    (3, *gf_specs[\"sum-layers.weight.0\"]), \n",
    "    torch.randn(3, 256)\n",
    ")\n",
    "\n",
    "print(\"Weights shape:\", weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e166702",
   "metadata": {},
   "source": [
    "We can now register the gating functions on the compiler, which will take care of compiling the conditional circuit, keep track of which function to call and execute them efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2635a086",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirkit.pipeline import PipelineContext\n",
    "\n",
    "# Initialize a pipeline compilation context\n",
    "# Let's try _without_ folding first\n",
    "ctx = PipelineContext(semiring=\"lse-sum\", backend='torch', fold=False, optimize=False)\n",
    "\n",
    "# Register our neural network as an external model\n",
    "ctx.add_gate_function(\"sum-layers.weight.0\", partial(random_sum_weights, gf_specs[\"sum-layers.weight.0\"]))\n",
    "ctx.add_gate_function(\"sum-layers.weight.1\", partial(random_sum_weights, gf_specs[\"sum-layers.weight.1\"]))\n",
    "\n",
    "# Finally, we compile the conditional circuit\n",
    "circuit = ctx.compile(symbolic_conditional_circuit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456732e3",
   "metadata": {},
   "source": [
    "And evaluate the conditional circuit by specifying the argument for each gating function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddb66bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-4357.3911]],\n",
       "\n",
       "        [[-4362.3804]],\n",
       "\n",
       "        [[-4361.9121]],\n",
       "\n",
       "        [[-4357.7598]],\n",
       "\n",
       "        [[-4358.0020]],\n",
       "\n",
       "        [[-4352.8735]],\n",
       "\n",
       "        [[-4358.7144]],\n",
       "\n",
       "        [[-4353.7617]],\n",
       "\n",
       "        [[-4352.5952]],\n",
       "\n",
       "        [[-4359.7930]]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randint(256, size=(10, 784))  # The circuit input\n",
    "z = torch.randn(size=(10, 127))  # Some dummy input to the neural net\n",
    "\n",
    "# Evaluate the circuit on some input\n",
    "# Note that we also pass some input to the external model\n",
    "circuit(x, gate_function_kwargs={'sum-layers.weight.0': {'z': z}, 'sum-layers.weight.1': {'z': z}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9d2ac4-0b13-4d44-9634-cdb8e2d58cf1",
   "metadata": {},
   "source": [
    "The above parameterization is robust to change in compilation flages, e.g., we can enable folding and layer optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0173f15-e819-496d-a376-97d6b765cb22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-4354.7549]],\n",
       "\n",
       "        [[-4353.4976]],\n",
       "\n",
       "        [[-4355.6870]],\n",
       "\n",
       "        [[-4361.1377]],\n",
       "\n",
       "        [[-4352.8804]],\n",
       "\n",
       "        [[-4354.7881]],\n",
       "\n",
       "        [[-4356.4692]],\n",
       "\n",
       "        [[-4356.9868]],\n",
       "\n",
       "        [[-4356.1367]],\n",
       "\n",
       "        [[-4359.3481]]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# folding and optimization is enabled\n",
    "ctx = PipelineContext(semiring=\"lse-sum\", backend='torch', fold=True, optimize=True)\n",
    "\n",
    "ctx.add_gate_function(\"sum-layers.weight.0\", partial(random_sum_weights, gf_specs[\"sum-layers.weight.0\"]))\n",
    "ctx.add_gate_function(\"sum-layers.weight.1\", partial(random_sum_weights, gf_specs[\"sum-layers.weight.1\"]))\n",
    "\n",
    "circuit = ctx.compile(symbolic_conditional_circuit)\n",
    "\n",
    "circuit(x, gate_function_kwargs={'sum-layers.weight.0': {'z': z}, 'sum-layers.weight.1': {'z': z}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38970c95",
   "metadata": {},
   "source": [
    "The conditional parametrization is batch-dependant: for each batch we independently parametrize the model. \n",
    "\n",
    "We can see how this influences the output by producing illegal weight parameters on purpose. The output of the circuit will not be a valid probability distribution anymore. To do so, we set all zero weigths for the first element in the batch. Intuitively, we should see the circuit producing a *stange* likelihood on the first batch and working regularly on the other.\n",
    "\n",
    "To do so, however, we have to disable the parameter activation from the symbolic circuit, otherwise the softmax activation would act as a guard, producing a valid probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ab8e16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter graph for a sum layer in the conditioned circuit:\n",
      "\t GateFunctionParameter(shape=(64, 64), name='sum-layers.weight.0', index=0)\n"
     ]
    }
   ],
   "source": [
    "from cirkit.templates.utils import Parameterization\n",
    "\n",
    "symbolic_circuit = data_modalities.image_data(\n",
    "    (1, 28, 28),\n",
    "    region_graph='quad-tree-4',\n",
    "    input_layer='categorical',\n",
    "    num_input_units=64,\n",
    "    sum_product_layer='cp',\n",
    "    num_sum_units=64,\n",
    "    sum_weight_param=Parameterization(activation=\"none\"), # disable the softmax activation\n",
    ")\n",
    "\n",
    "symbolic_conditional_circuit, gf_specs = SF.condition_circuit(\n",
    "    symbolic_circuit,                          # The symbolic circut we want to parameterize\n",
    "    gate_functions={\n",
    "        \"sum-layers\": list(symbolic_circuit.sum_layers)\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Parameter graph for a sum layer in the conditioned circuit:\")\n",
    "for n in next(symbolic_conditional_circuit.sum_layers).params[\"weight\"].nodes:\n",
    "    print(\"\\t\", n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551b9d04",
   "metadata": {},
   "source": [
    "We can see that now the sum weights are parameterized by the gating function on its own, which is single-handedly responsible for the normalization step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "187eff40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[      -inf]],\n",
       "\n",
       "        [[-4364.7510]],\n",
       "\n",
       "        [[-4356.0972]],\n",
       "\n",
       "        [[-4359.3677]],\n",
       "\n",
       "        [[-4361.9619]],\n",
       "\n",
       "        [[-4366.1802]],\n",
       "\n",
       "        [[-4354.0938]],\n",
       "\n",
       "        [[-4353.7583]],\n",
       "\n",
       "        [[-4350.3306]],\n",
       "\n",
       "        [[-4357.8169]]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def random_sum_weights_zero_first_sample(shape, z: torch.Tensor):\n",
    "    # compute the mean and standard deviation of all the elements in the batch\n",
    "    mean, stddev = torch.mean(z, dim=-1), torch.std(z, dim=-1)\n",
    "    # compute weights by randomly sampling\n",
    "    samples = torch.randn(*shape)\n",
    "    weight = mean.view(-1, 1, 1, 1) + stddev.view(-1, 1, 1, 1) * samples\n",
    "    \n",
    "    # manually apply the softmax activation\n",
    "    weight = torch.softmax(weight, dim=-1)\n",
    "\n",
    "    # set first element in batch to 0\n",
    "    weight[0] = 0\n",
    "    return weight\n",
    "\n",
    "# register the new gate function and compile the circuit\n",
    "ctx = PipelineContext(semiring=\"lse-sum\", backend='torch', fold=True, optimize=True)\n",
    "ctx.add_gate_function(\"sum-layers.weight.0\", partial(random_sum_weights_zero_first_sample, gf_specs[\"sum-layers.weight.0\"]))\n",
    "ctx.add_gate_function(\"sum-layers.weight.1\", partial(random_sum_weights_zero_first_sample, gf_specs[\"sum-layers.weight.1\"]))\n",
    "circuit = ctx.compile(symbolic_conditional_circuit)\n",
    "\n",
    "# run the circuit on the same dummy inputs\n",
    "circuit(x, gate_function_kwargs={'sum-layers.weight.0': {'z': z}, 'sum-layers.weight.1': {'z': z}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9947e28",
   "metadata": {},
   "source": [
    "And indeed, the first batch evaluates to a negative log likelihood equal to $-\\infty$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cirkit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
