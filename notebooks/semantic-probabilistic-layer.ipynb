{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f61638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Set some seeds\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# Set the torch device to use\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eea1ec1",
   "metadata": {},
   "source": [
    "## Semantic Probabilistic Layer using `cirkit`\n",
    "\n",
    "In the [Semantic Loss](../semantic-loss) notebook we saw how it is possible to implement neuro-symbolic methods using `cirkit`. In that case, the circuit was used as a regularization to a standard loss, *encouraging* consistent predictions according to some logical constraint $\\phi$.\n",
    "\n",
    "In this notebook we implement the **semantic probabilistic layer** (SPL) [(Ahmed et al, 2022)](https://arxiv.org/abs/2206.00426), which takes a different approach by exploiting the combination between a PC and a logical circuit.\n",
    "\n",
    "Recall that given a logical constraint $\\phi$, we can compile it to a circuit and implement it using `cirkit`. The very same circuit, however, is already a probabilistic circuit: it just defines a uniform unnormalized probability distribution over those assignments that are models of $\\phi$ and enforces a null probability to every other assignment. This is due to the unitary weights over the sum units.\n",
    "\n",
    "By changing the parameters of PC, we are able to *condition* the probability distribution of the circuit based on some external observation [(Shao et al, 2022)](https://www.sciencedirect.com/science/article/pii/S0888613X21001766). By doing so, however, the support of the circuit will be the same: only models of $\\phi$ can have a non-null probability.\n",
    "\n",
    "SPL leverage conditional circuits by training the PC on the target labels of a classification task, for instance MNIST images, and parameterizes the PC using a neural network that takes as input the MNIST image and output a parameter configuration for the PC.\n",
    "During inference, it is possible to obtain the prediction for an image by conditionally parameterizing the PC and computing its MAP state.\n",
    "\n",
    "In this notebook, we will show how to implement SPL by using a simple obvious constraint over MNIST images: the fact that each image only belongs to one class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a7217e",
   "metadata": {},
   "source": [
    "We first construct the logical constraint $\\phi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84d239bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import and_, or_\n",
    "from functools import reduce\n",
    "from itertools import combinations\n",
    "\n",
    "from pysdd.sdd import Vtree, SddManager\n",
    "\n",
    "N = 10\n",
    "\n",
    "# define the SDD literals\n",
    "vtree = Vtree(N, list(range(1, N + 1)), \"balanced\")\n",
    "manager = SddManager.from_vtree(vtree)\n",
    "alpha = reduce(or_, manager.vars) & reduce(and_, (~a | ~b for a, b in combinations(manager.vars, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f561daa",
   "metadata": {},
   "source": [
    "And compile it into a circuit using `cirkit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88acc32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempfile import NamedTemporaryFile\n",
    "from cirkit.templates.logic import SDD\n",
    "from cirkit.symbolic.layers import EmbeddingLayer\n",
    "from cirkit.symbolic.parameters import Parameter, ConstantParameter\n",
    "from cirkit.symbolic.io import plot_circuit\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "with NamedTemporaryFile() as f:\n",
    "    # export the SDD to a file\n",
    "    alpha.save(f.name.encode())\n",
    "    f.flush()\n",
    "\n",
    "    # parse the SDD representation using the SDD class\n",
    "    alpha_sdd = SDD.from_file(f.name)\n",
    "\n",
    "    # we make sure that the sum layers include a softmax activation in their\n",
    "    # parameter graph\n",
    "    alpha_symbolic = alpha_sdd.build_circuit(sum_weight_activation=\"softmax\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ccc611",
   "metadata": {},
   "source": [
    "In this case, for the sake of simplicity, we will only parameterize the sum units of the resulting PC, as this allows to train a single neural network to predict the parameters of the PC. Nonetheless, different combination of parameters (including input units) can be externally parameterized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "821e0c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters sum.weight.0 need shape (9, 1, 2)\n"
     ]
    }
   ],
   "source": [
    "from cirkit.templates.logic import LiteralNode, NegatedLiteralNode\n",
    "from cirkit.symbolic.functional import condition_circuit\n",
    "\n",
    "# we enforce an order on the literals to match the order of the predictions\n",
    "parametrization_map = {\n",
    "    \"sum\": list(set(alpha_symbolic.sum_layers))\n",
    "}\n",
    "\n",
    "# conditionally parametrize alpha by using the external gate function\n",
    "conditional_alpha_symbolic, gf_specs = condition_circuit(alpha_symbolic, gate_functions=parametrization_map)\n",
    "\n",
    "for gf_k, gf_shape in gf_specs.items():\n",
    "    print(f\"Parameters {gf_k} need shape {gf_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38f47b7",
   "metadata": {},
   "source": [
    "We can now compile the circuit into a torch computational graph. Note that in order to obtain a convex combination of input units in sums, we apply a softmax activation to each sum units' parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ae705ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from cirkit.pipeline import PipelineContext\n",
    "\n",
    "ctx = PipelineContext(backend=\"torch\", semiring=\"lse-sum\", fold=True)\n",
    "ctx.add_gate_function(\"sum.weight.0\", lambda x: x.view(-1, *gf_specs[\"sum.weight.0\"]))\n",
    "circuit = ctx.compile(conditional_alpha_symbolic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05737625",
   "metadata": {},
   "source": [
    "We can now train the circuit on the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2695d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "from operator import mul\n",
    "\n",
    "from cirkit.backend.torch.queries import MAPQuery\n",
    "\n",
    "# Load the MNIST data set and data loaders\n",
    "transform = v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])\n",
    "data_train = datasets.MNIST('datasets', train=True, download=True, transform=transform)\n",
    "data_test = datasets.MNIST('datasets', train=False, download=True, transform=transform)\n",
    "\n",
    "# Instantiate the training and testing data loaders\n",
    "train_dataloader = DataLoader(data_train, shuffle=True, batch_size=256)\n",
    "test_dataloader = DataLoader(data_test, shuffle=False, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678161d4",
   "metadata": {},
   "source": [
    "We will first train a baseline MLP model with hidden one layer and 256 hidden units for 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674917d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 0.22972959280014038 | Accuracy: 0.964\n",
      "Epoch: 1 | Loss: 0.09498841315507889 | Accuracy: 0.976\n",
      "Epoch: 2 | Loss: 0.07156120985746384 | Accuracy: 0.973\n",
      "Epoch: 3 | Loss: 0.05734336003661156 | Accuracy: 0.975\n",
      "Epoch: 4 | Loss: 0.048394735902547836 | Accuracy: 0.976\n",
      "Epoch: 5 | Loss: 0.04697965085506439 | Accuracy: 0.974\n",
      "Epoch: 6 | Loss: 0.037493132054805756 | Accuracy: 0.977\n",
      "Epoch: 7 | Loss: 0.0397108793258667 | Accuracy: 0.975\n",
      "Epoch: 8 | Loss: 0.03173552453517914 | Accuracy: 0.974\n",
      "Epoch: 9 | Loss: 0.02695915289223194 | Accuracy: 0.973\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "running_loss = 0\n",
    "running_samples = 0\n",
    "\n",
    "mlp = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(28 * 28, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 10)\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# Initialize a torch optimizer of your choice\n",
    "optimizer = optim.Adam(mlp.parameters(), lr=0.01)\n",
    "\n",
    "for epoch_idx in range(num_epochs):\n",
    "    for i, (x, y) in enumerate(train_dataloader):\n",
    "        # compute the predictions using the MLP\n",
    "        preds = mlp(x.to(device))\n",
    "\n",
    "        loss = torch.nn.functional.cross_entropy(preds, y.to(device))\n",
    "\n",
    "        # Update the parameters of the circuits, as any other model in PyTorch\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        running_loss += loss * len(x)\n",
    "        running_samples += len(x)\n",
    "    \n",
    "    average_nl = running_loss / running_samples\n",
    "        \n",
    "    # compute the accuracy and logical correctness on the testing set\n",
    "    correct_predictions = 0.0\n",
    "    logically_correct_predictions = 0.0\n",
    "    for x, y in test_dataloader:\n",
    "        with torch.no_grad():\n",
    "            pred = mlp(x.to(device))\n",
    "            correct_predictions += (pred.argmax(dim=-1) == y.to(device)).sum()\n",
    "    \n",
    "    accuracy = correct_predictions / len(data_test)\n",
    "    \n",
    "    print(f\"Epoch: {epoch_idx} | Loss: {average_nl} | Accuracy: {accuracy:.3f}\")\n",
    "    running_loss = 0.0\n",
    "    running_samples = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4bbfca",
   "metadata": {},
   "source": [
    "Let's train the SPL model now. We will use a similar MLP model to predict the parameters of each sum: one hidden layers with 256 units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0c22897",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(28 * 28, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, reduce(mul, gf_specs[\"sum.weight.0\"]))\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# Initialize a torch optimizer of your choice\n",
    "optimizer = optim.Adam(mlp.parameters(), lr=0.01)\n",
    "\n",
    "# move circuit to device\n",
    "circuit = circuit.to(device)\n",
    "\n",
    "# prepare the map query to extract predictions\n",
    "map_query = MAPQuery(circuit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d801ceff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | NL: 0.27490881085395813 | Accuracy: 0.959\n",
      "Epoch: 1 | NL: 0.12324030697345734 | Accuracy: 0.960\n",
      "Epoch: 2 | NL: 0.08998344838619232 | Accuracy: 0.964\n",
      "Epoch: 3 | NL: 0.07511863857507706 | Accuracy: 0.963\n",
      "Epoch: 4 | NL: 0.06081372871994972 | Accuracy: 0.967\n",
      "Epoch: 5 | NL: 0.058237019926309586 | Accuracy: 0.972\n",
      "Epoch: 6 | NL: 0.051081474870443344 | Accuracy: 0.966\n",
      "Epoch: 7 | NL: 0.050264522433280945 | Accuracy: 0.963\n",
      "Epoch: 8 | NL: 0.04702281579375267 | Accuracy: 0.968\n",
      "Epoch: 9 | NL: 0.04019814357161522 | Accuracy: 0.967\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "running_loss = 0\n",
    "running_samples = 0\n",
    "\n",
    "for epoch_idx in range(num_epochs):\n",
    "    for i, (x, y) in enumerate(train_dataloader):\n",
    "        # compute the predictions using the MLP\n",
    "        likelihoods = circuit(\n",
    "            torch.nn.functional.one_hot(y.to(device), num_classes=10),\n",
    "            gate_function_kwargs={\n",
    "                \"sum.weight.0\": {\"x\": mlp(x.to(device))},\n",
    "         })\n",
    "\n",
    "        loss = -likelihoods.mean()\n",
    "\n",
    "        # Update the parameters of the circuits, as any other model in PyTorch\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        running_loss += loss * len(x)\n",
    "        running_samples += len(x)\n",
    "    \n",
    "    average_nl = running_loss / running_samples\n",
    "        \n",
    "    # compute the accuracy and logical correctness on the testing set\n",
    "    correct_predictions = 0.0\n",
    "    logically_correct_predictions = 0.0\n",
    "    for x, y in test_dataloader:\n",
    "        with torch.no_grad():\n",
    "            _, pred = map_query(gate_function_kwargs={\n",
    "                \"sum.weight.0\": {\"x\": mlp(x.to(device))},\n",
    "            })\n",
    "            correct_predictions += (pred.argmax(dim=-1) == y.to(device)).sum()\n",
    "    \n",
    "    accuracy = correct_predictions / len(data_test)\n",
    "    \n",
    "    print(f\"Epoch: {epoch_idx} | NL: {average_nl} | Accuracy: {accuracy:.3f}\")\n",
    "    running_loss = 0.0\n",
    "    running_samples = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cirkit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
