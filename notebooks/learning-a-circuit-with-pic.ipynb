{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1e6d6b9",
   "metadata": {},
   "source": [
    "# Probabilistic Integral Circuits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bf2b04-012a-4d5f-8700-39e848f6b2c9",
   "metadata": {},
   "source": [
    "In this notebook, we will show an alternative way of learning the parameters of tensorized folded (probabilistic) circuits.\n",
    "\n",
    "This technique is actually based on another model class called *Probabilistic Integral Circuit* (PIC), which extends Probabilistic Circuits (PCs) by adding *integral units*, which allow modelling continuous latent variables.\n",
    "\n",
    "Fortunately enough, we do **not** need to fully understand PICs to apply them! In fact, from an application point of view, all we need to do is replacing every folded tensor parameter with a neural net whose output is an equally-sized tensor! Therefore, the actual parameters we are going to optimize are those of such neural nets, and not the original tensors. This is it -- nothing less, nothing more.\n",
    "\n",
    "To showcase this alternative parameter learning scheme, we will first instantiate a folded circuit as shown in ```learning-a-circuit.ipynb```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52c88f38-1552-4d13-b62d-931493c07c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirkit.templates import circuit_templates\n",
    "from cirkit.pipeline import compile\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "# Set some seeds\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# Set the torch device to use\n",
    "device = torch.device('cuda')\n",
    "\n",
    "symbolic_circuit = circuit_templates.image_data(\n",
    "    (1, 28, 28),                # The shape of MNIST image, i.e., (num_channels, image_height, image_width)\n",
    "    region_graph='quad-graph',  # Select the structure of the circuit to follow the QuadGraph region graph\n",
    "    input_layer='categorical',  # Use Categorical distributions for the pixel values (0-255) as input layers\n",
    "    num_input_units=64,         # Each input layer consists of 64 Categorical input units\n",
    "    sum_product_layer='cp',     # Use CP sum-product layers, i.e., alternate dense sum layers and hadamard product layers\n",
    "    num_sum_units=64,           # Each dense sum layer consists of 64 sum units\n",
    "    sum_weight_param=circuit_templates.Parameterization(\n",
    "        activation='none',      # Do not use any parameterization\n",
    "        initialization='normal' # Initialize the sum weights by sampling from a standard normal distribution\n",
    "    )\n",
    ")\n",
    "circuit = compile(symbolic_circuit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cd388c",
   "metadata": {},
   "source": [
    "The one above is the very same circuit from ```learning-a-circuit.ipynb```. Let's now print some stuff related to its first and second layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35046e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TorchCategoricalLayer(\n",
      "  folds: 784  channels: 1  variables: 1  output-units: 64\n",
      "  input-shape: (784, 1, -1, 1)\n",
      "  output-shape: (784, -1, 64)\n",
      "  (probs): TorchParameter(\n",
      "    shape: (784, 64, 1, 256)\n",
      "    (0): TorchTensorParameter(output-shape: (784, 64, 1, 256))\n",
      "    (1): TorchSoftmaxParameter(\n",
      "      input-shapes: [(784, 64, 1, 256)]\n",
      "      output-shape: (784, 64, 1, 256)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "torch.Size([784, 64, 1, 256])\n"
     ]
    }
   ],
   "source": [
    "print(circuit.nodes[0])\n",
    "print(circuit.nodes[0].probs().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2145a9b8",
   "metadata": {},
   "source": [
    "The first layer is an input categorical layer, which models the 784 gray-scale pixels using a folded tensor of shape (784, 64, 1, 256)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6688610d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TorchSumLayer(\n",
      "  folds: 1568  arity: 1  input-units: 64  output-units: 64\n",
      "  input-shape: (1568, 1, -1, 64)\n",
      "  output-shape: (1568, -1, 64)\n",
      "  (weight): TorchParameter(\n",
      "    shape: (1568, 64, 64)\n",
      "    (0): TorchTensorParameter(output-shape: (1568, 64, 64))\n",
      "  )\n",
      ")\n",
      "torch.Size([1568, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "print(circuit.nodes[1])\n",
    "print(circuit.nodes[1].weight().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b435d7be",
   "metadata": {},
   "source": [
    "The second layer is a TorchDenseLayer whose shape is (1568, 64, 64)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a8a724",
   "metadata": {},
   "source": [
    "## Converting PCs to Quadrature PCs (QPCs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7d74fd",
   "metadata": {},
   "source": [
    "As we mentioned earlier, all we need to do is replacing every folded tensor parameter with a neural net whose output returns an equally-sized tensor. Therefore, instead of training tensors, we will be training neural nets that output tensors.\n",
    "\n",
    "We can do such *replacement* in one line by using the API ```pc2qpc```. Here QPC stands for *Quadrature* PC, and it is simply a discretization of the underlying PIC which is parameterized by neural nets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3989ee36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirkit.backend.torch.parameters.pic import pc2qpc\n",
    "pc2qpc(circuit, integration_method=\"trapezoidal\", net_dim=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ccd6fe",
   "metadata": {},
   "source": [
    "Let's now inspect again the first layer of ```circuit```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8113608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TorchCategoricalLayer(\n",
      "  folds: 784  channels: 1  variables: 1  output-units: 64\n",
      "  input-shape: (784, 1, -1, 1)\n",
      "  output-shape: (784, -1, 64)\n",
      "  (probs): PICInputNet(\n",
      "    (reparam): TorchSoftmaxParameter(\n",
      "      input-shapes: [(784, 64, 1, 256)]\n",
      "      output-shape: (784, 64, 1, 256)\n",
      "    )\n",
      "    (net): Sequential(\n",
      "      (0): FourierLayer(1, 256, sigma=1.0)\n",
      "      (1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (2): Tanh()\n",
      "      (3): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "  )\n",
      ")\n",
      "torch.Size([784, 64, 1, 256])\n"
     ]
    }
   ],
   "source": [
    "print(circuit.nodes[0])\n",
    "print(circuit.nodes[0].probs().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386236bc",
   "metadata": {},
   "source": [
    "We see that the first layer is now parameterized by a neural net (instance of the class ```PICInputNet```), and that its evaluation delivers an equally-shaped tensor as before, i.e. (784, 64, 1, 256).\n",
    "\n",
    "Let's check now the second layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "641720cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TorchSumLayer(\n",
      "  folds: 1568  arity: 1  input-units: 64  output-units: 64\n",
      "  input-shape: (1568, 1, -1, 64)\n",
      "  output-shape: (1568, -1, 64)\n",
      "  (weight): PICInnerNet(\n",
      "    (net): Sequential(\n",
      "      (0): FourierLayer(2, 256, sigma=1.0)\n",
      "      (1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (2): Tanh()\n",
      "      (3): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (4): Tanh()\n",
      "      (5): Conv1d(401408, 1568, kernel_size=(1,), stride=(1,))\n",
      "      (6): Softplus(beta=1.0, threshold=20.0)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "torch.Size([1568, 64, 64])\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "print(circuit.nodes[1])\n",
    "print(circuit.nodes[1].weight().shape)\n",
    "print(torch.all(torch.isclose(circuit.nodes[1].weight().sum(-1), torch.tensor(1.0))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba32233",
   "metadata": {},
   "source": [
    "Similarly, the second layer is parameterized by a neural net (instance of the class ```PICInnerNet```), and its evaluation delivers an equally-shaped tensor as before, i.e. (1568, 64, 64). Finally, note that output tensors from neural nets associated to inner layers are already normalized, in the sense that they sum up to 1 over the last dimension, therefore leading to a PC with normalization constant equal to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe004cd",
   "metadata": {},
   "source": [
    "That is it, we are done! ðŸŽ‰ We can now even forget about PICs, and just train as in ```learning-a-circuit.ipynb``` as we do next. However, if you want to learn more about PICs (and understand the input arguments of ```pc2qpc```) please check: https://arxiv.org/abs/2406.06494."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beee1f04",
   "metadata": {},
   "source": [
    "## Learning a Probabilistic (Integral) Circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02854883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "# Load the MNIST data set and data loaders\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # Flatten the images and set pixel values in the [0-255] range\n",
    "    transforms.Lambda(lambda x: (255 * x.view(-1)).long())\n",
    "])\n",
    "data_train = datasets.MNIST('datasets', train=True, download=True, transform=transform)\n",
    "data_test = datasets.MNIST('datasets', train=False, download=True, transform=transform)\n",
    "\n",
    "# Instantiate the training and testing data loaders\n",
    "train_dataloader = DataLoader(data_train, shuffle=True, batch_size=256)\n",
    "test_dataloader = DataLoader(data_test, shuffle=False, batch_size=256)\n",
    "\n",
    "# Initialize a torch optimizer of your choice,\n",
    "#  e.g., Adam, by passing the parameters of the circuit\n",
    "optimizer = optim.Adam(circuit.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f28e9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 200: Average NLL: 798.951\n",
      "Step 400: Average NLL: 704.324\n",
      "Step 600: Average NLL: 681.021\n",
      "Step 800: Average NLL: 669.410\n",
      "Step 1000: Average NLL: 660.258\n",
      "Step 1200: Average NLL: 655.782\n",
      "Step 1400: Average NLL: 653.661\n",
      "Step 1600: Average NLL: 651.993\n",
      "Step 1800: Average NLL: 651.169\n",
      "Step 2000: Average NLL: 649.297\n",
      "Step 2200: Average NLL: 648.773\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "step_idx = 0\n",
    "running_loss = 0.0\n",
    "running_samples = 0\n",
    "\n",
    "# Move the circuit to chosen device\n",
    "circuit = circuit.to(device)\n",
    "\n",
    "for epoch_idx in range(num_epochs):\n",
    "    for i, (batch, _) in enumerate(train_dataloader):\n",
    "        # The circuit expects an input of shape (batch_dim, num_channels, num_variables),\n",
    "        # so we unsqueeze a dimension for the channel.\n",
    "        batch = batch.to(device).unsqueeze(dim=1)\n",
    "\n",
    "        # Compute the log-likelihoods of the batch, by evaluating the circuit\n",
    "        log_likelihoods = circuit(batch)\n",
    "\n",
    "        # We take the negated average log-likelihood as loss\n",
    "        loss = -torch.mean(log_likelihoods)\n",
    "        loss.backward()\n",
    "        # Update the parameters of the circuits, as any other model in PyTorch\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        running_loss += loss.detach() * len(batch)\n",
    "        running_samples += len(batch)\n",
    "        step_idx += 1\n",
    "        if step_idx % 200 == 0:\n",
    "            average_nll = running_loss / running_samples\n",
    "            print(f\"Step {step_idx}: Average NLL: {average_nll:.3f}\")\n",
    "            running_loss = 0.0\n",
    "            running_samples = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50efae6b",
   "metadata": {},
   "source": [
    "We evaluate our probabilistic circuit on the test data by computing the average log-likelihood and bits per dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e66bd8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test LL: -643.881\n",
      "Bits per dimension: 1.185\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    test_lls = 0.0\n",
    "\n",
    "    for batch, _ in test_dataloader:\n",
    "        # The circuit expects an input of shape (batch_dim, num_channels, num_variables),\n",
    "        # so we unsqueeze a dimension for the channel.\n",
    "        batch = batch.to(device).unsqueeze(dim=1)\n",
    "\n",
    "        # Compute the log-likelihoods of the batch\n",
    "        log_likelihoods = circuit(batch)\n",
    "\n",
    "        # Accumulate the log-likelihoods\n",
    "        test_lls += log_likelihoods.sum().item()\n",
    "\n",
    "    # Compute average test log-likelihood and bits per dimension\n",
    "    average_ll = test_lls / len(data_test)\n",
    "    bpd = -average_ll / (28 * 28 * np.log(2.0))\n",
    "    print(f\"Average test LL: {average_ll:.3f}\")\n",
    "    print(f\"Bits per dimension: {bpd:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
