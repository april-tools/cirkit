{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How-To: Add an Operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook introduces how to add a custom input layer.\n",
    "\n",
    "- For users: There's no way to do it as the end-user. Currently it's designed that modifications to the library code is required.\n",
    "- For developers: Please look at comments for each code block to decide where to add/modify the code pieces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new operator requires symbolic definition(s) for each layer supporting it and the implementation(s) corresponding to the backend(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will illustrate the process with `MYDIFFERENTIATION` and its `torch` backend, which is a replicate of `DIFFERENTIATION` in the library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we need to register the new operator in two `Enum` class: `CircuitOperator` and `LayerOperator`.\n",
    "\n",
    "This can only be done by modifying the corresponding classes in the library, but not possible in the user code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In rare cases that an operator on circuit does not involve any transformation of the layers (e.g. `CircuitOperator.CONCATENATE`), it may skip the `LayerOperator` class and the implementation for the operator on the layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[cirkit/symbolic/circuit.py](../cirkit/symbolic/circuit.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import IntEnum, auto\n",
    "\n",
    "\n",
    "class CircuitOperator(IntEnum):\n",
    "    ...  # Any existing enum values.\n",
    "    MYDIFFERENTIATION = auto()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[cirkit/symbolic/layers.py](../cirkit/symbolic/layers.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerOperator(IntEnum):\n",
    "    ...  # Any existing enum values.\n",
    "    MYDIFFERENTIATION = auto()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symbolic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the symbolic part, we will have to:\n",
    "- Decide the layers that supports this operator;\n",
    "  - Identify the parameter operations required by the operator;\n",
    "- Define the process to operate a circuit with its symbolic representation.\n",
    "\n",
    "All the above will not involve any actual tensors, just the configs and shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add as many layers as the operator supports, but for illustrative purposes, here we only illustrate with `PolynomialLayer` in the library.\n",
    "\n",
    "For layers that do not support the operator, just leave it out and it will be properly handled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After deciding which layer(s) we want to support, we must define the parameter operations the layer(s) need(s).\n",
    "\n",
    "Since we are only looking at `PolynomialLayer` here, and the layer only has one parameter `coeff`, we only need to define one patameter operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As multiplication is a unary operator, we can inherit from `UnaryParameterOp` to make the best use of existing infrastructure. Alternatively, a more general `ParameterOp` class may be inherited.\n",
    "\n",
    "The mininum definition should include the `shape` property which defines the output shape of this parameter operation.\n",
    "\n",
    "In this case, we also need `__init__` as differentiation also needs an additional argument `order`, and `config` should also be overriden to include `order`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[cirkit/symbolic/parameters.py](../cirkit/symbolic/parameters.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "from cirkit.symbolic.parameters import UnaryParameterOp\n",
    "\n",
    "\n",
    "class PolynomialMyDifferential(UnaryParameterOp):\n",
    "    def __init__(self, in_shape: tuple[int, ...], *, order: int = 1):\n",
    "        if order <= 0:\n",
    "            raise ValueError(\"The order of differentiation must be positive.\")\n",
    "        super().__init__(in_shape)\n",
    "        self.order = order\n",
    "\n",
    "    @property\n",
    "    def shape(self) -> tuple[int, ...]:\n",
    "        # if dp1>order, i.e., deg>=order, then diff, else const 0.\n",
    "        return (\n",
    "            self.in_shapes[0][0],  # dim Ko\n",
    "            self.in_shapes[0][1] - self.order\n",
    "            if self.in_shapes[0][1] > self.order\n",
    "            else 1,  # dim dp1\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def config(self) -> dict[str, Any]:\n",
    "        return {**super().config, \"order\": self.order}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the param op has been defined, we can then define how an operator act on the layer by defining a rule function and registering it to the rules registry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to share the underlying parameters across the operations, `param.ref()` should be passed to build the new parameter from the operators.\n",
    "\n",
    "And then, the resulting new layer (or can be layers, if needed) should be wrapped in a `CircuitBlock` for return."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[cirkit/symbolic/operators.py](../cirkit/symbolic/operators.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirkit.symbolic.circuit import CircuitBlock\n",
    "from cirkit.symbolic.layers import PolynomialLayer\n",
    "from cirkit.symbolic.operators import DEFAULT_OPERATOR_RULES\n",
    "from cirkit.symbolic.parameters import Parameter\n",
    "\n",
    "\n",
    "def my_differentiate_polynomial_layer(\n",
    "    sl: PolynomialLayer, *, var_idx: int, ch_idx: int, order: int = 1\n",
    ") -> CircuitBlock:\n",
    "    # PolynomialLayer is constructed univariate, but we still take the 2 idx for unified interface\n",
    "    assert (var_idx, ch_idx) == (0, 0), \"This should not happen\"\n",
    "    if order <= 0:\n",
    "        raise ValueError(\"The order of differentiation must be positive.\")\n",
    "    coeff = Parameter.from_unary(\n",
    "        PolynomialMyDifferential(sl.coeff.shape, order=order), sl.coeff.ref()\n",
    "    )\n",
    "    sl = PolynomialLayer(\n",
    "        sl.scope, sl.num_output_units, sl.num_channels, degree=coeff.shape[-1] - 1, coeff=coeff\n",
    "    )\n",
    "    return CircuitBlock.from_layer(sl)\n",
    "\n",
    "\n",
    "DEFAULT_OPERATOR_RULES[LayerOperator.MYDIFFERENTIATION].append(my_differentiate_polynomial_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operation on Symbolic Circuit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement the operator in a symbolic way, we need to define a function that takes in the symbolic circuit(s) and a optional custom registry, along with any other args the operator needs. The function should return the resulting circuit after applying the operator, with proper parameter sharing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code, we omit many algorithmic details, but focus on points to note with coding. Please read the comments for how it's expected to be defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[cirkit/symbolic/functional.py](../cirkit/symbolic/functional.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections.abc import Sequence\n",
    "\n",
    "from cirkit.symbolic.circuit import Circuit, CircuitOperation, StructuralPropertyError\n",
    "from cirkit.symbolic.layers import InputLayer, Layer, SumLayer\n",
    "from cirkit.symbolic.registry import OPERATOR_REGISTRY, OperatorRegistry\n",
    "\n",
    "\n",
    "# sc (or multiple if n-ary op) and registry is common interface, order is specific to diff.\n",
    "def my_differentiate(\n",
    "    sc: Circuit, registry: OperatorRegistry | None = None, *, order: int = 1\n",
    ") -> Circuit:\n",
    "    # Sanity checks of args.\n",
    "    if not sc.is_smooth or not sc.is_decomposable:\n",
    "        raise StructuralPropertyError(\n",
    "            \"Only smooth and decomposable circuits can be efficiently differentiated.\"\n",
    "        )\n",
    "    if order <= 0:\n",
    "        raise ValueError(\"The order of differentiation must be positive.\")\n",
    "\n",
    "    # Use the registry in the current context, if not specified otherwise.\n",
    "    if registry is None:\n",
    "        registry = OPERATOR_REGISTRY.get()\n",
    "\n",
    "    # Keep a mapping from the layers in the input circuit to the blocks in the output circuit.\n",
    "    # Depending on the algorithm, another form of mapping may be used.\n",
    "    layers_to_blocks: dict[Layer, list[CircuitBlock]] = {}\n",
    "\n",
    "    # The directed edges connecting the blocks in the output circuit, as a mapping from each block\n",
    "    # to its inputs. This must be defined in this form to construct the output circuit.\n",
    "    in_blocks: dict[CircuitBlock, Sequence[CircuitBlock]] = {}\n",
    "\n",
    "    # Iterate all the symbolic layers in the input circuit in the topological order.\n",
    "    for sl in sc.topological_ordering():\n",
    "        # For an InputLayer, e.g. PolynomialLayer, the rule should exist in the registry.\n",
    "        if isinstance(sl, InputLayer):\n",
    "            # Retrieve the differentiation rule from the registry.\n",
    "            func = registry.retrieve_rule(LayerOperator.MYDIFFERENTIATION, type(sl))\n",
    "            # Get the differential using the rule.\n",
    "            diff_blocks = [func(sl, var_idx=0, ch_idx=0, order=order)]\n",
    "\n",
    "            # Save the blocks as corresponding to the current symbolic layer.\n",
    "            layers_to_blocks[sl] = diff_blocks\n",
    "\n",
    "            # Update to in_blocks can be omitted: blocks not exist will be treated as no input.\n",
    "            # in_blocks[diff_blocks[0]] = []\n",
    "\n",
    "        # For a SumLayer, the original connectivity and params are copied in the differential.\n",
    "        elif isinstance(sl, SumLayer):\n",
    "            # An idiom to make a copy of a layer and keep the params shared.\n",
    "            diff_blocks = [\n",
    "                CircuitBlock.from_layer(\n",
    "                    type(sl)(**sl.config, **{name: p.ref() for name, p in sl.params.items()})\n",
    "                )\n",
    "            ]\n",
    "            # Each item corresponds to the item in diff_blocks, meaning its inputs are the blocks\n",
    "            # corresponding to the inputs of the current layer.\n",
    "            diff_in_blocks = [\n",
    "                [layers_to_blocks[sl_in][0] for sl_in in sc.layer_inputs(sl)],\n",
    "            ]\n",
    "\n",
    "            # Save the blocks as corresponding to the current symbolic layer.\n",
    "            layers_to_blocks[sl] = diff_blocks\n",
    "\n",
    "            # Update in_blocks with the blocks and the corresponding inputs.\n",
    "            in_blocks.update(zip(diff_blocks, diff_in_blocks))\n",
    "\n",
    "        # There can be other cases processed based on need.\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    # End of `for sl in sc.topological_ordering():`\n",
    "\n",
    "    # Construct the differential symbolic circuit and set the differentiation operation metadata.\n",
    "    return Circuit.from_operation(\n",
    "        sc.scope,  # The scope is the same, or may change based on the algorithm.\n",
    "        sc.num_channels,  # Channels shall not change in most cases.\n",
    "        itertools.chain.from_iterable(layers_to_blocks.values()),  # All the blocks constructed.\n",
    "        in_blocks,  # The edges as recoded.\n",
    "        itertools.chain.from_iterable(layers_to_blocks[sl] for sl in sc.outputs),  # Idiom.\n",
    "        operation=CircuitOperation(  # Metadata of the operation.\n",
    "            operator=CircuitOperator.MYDIFFERENTIATION,  # The Enum value for the op.\n",
    "            operands=(sc,),  # The operands, anything passed into this operator function.\n",
    "            metadata=dict(order=order),  # Any additional args of the operator.\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation with Backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the backend implementation, we will have to:\n",
    "- Implement the actual computation for the layer and operator(s);\n",
    "- Specify the rule that maps the implementation above with the symbolic layer/operator(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What has been provided in the symbolic part should has a corresponding implmentation with the backend, although the rules are actually what handles whether and how the symbolic representation is translated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `torch` Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `torch` version of operators also provides a `TorchUnaryParameterOp` for easier implementation, with `TorchParameterOp` for more customization.\n",
    "\n",
    "The minimal implementation can include only the `shape` of output parameter, and the `forward` that transforms the input parameter(s) to the output.\n",
    "\n",
    "In this case, we also need `__init__` as differentiation also needs an additional argument `order`, and `config` should also be overriden to include `order`.\n",
    "\n",
    "And optionally, `fold_settings` can be provided to contain any additional shapes that affect folding (here the default is enough as the input shape decides everything)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[cirkit/backend/torch/parameters/nodes.py](../cirkit/backend/torch/parameters/nodes.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "from cirkit.backend.torch.parameters.nodes import TorchUnaryParameterOp\n",
    "\n",
    "\n",
    "class TorchPolynomialMyDifferential(TorchUnaryParameterOp):\n",
    "    def __init__(self, in_shape: tuple[int, ...], *, num_folds: int = 1, order: int = 1) -> None:\n",
    "        if order <= 0:\n",
    "            raise ValueError(\"The order of differentiation must be positive.\")\n",
    "        super().__init__(in_shape, num_folds=num_folds)\n",
    "        self.order = order\n",
    "\n",
    "    @property\n",
    "    def shape(self) -> tuple[int, ...]:\n",
    "        # if dp1>order, i.e., deg>=order, then diff, else const 0.\n",
    "        return (\n",
    "            self.in_shapes[0][0],  # dim Ko\n",
    "            self.in_shapes[0][1] - self.order\n",
    "            if self.in_shapes[0][1] > self.order\n",
    "            else 1,  # dim dp1\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def config(self) -> dict[str, Any]:\n",
    "        return {**super().config, \"order\": self.order}\n",
    "\n",
    "    def forward(self, coeff: Tensor) -> Tensor:\n",
    "        if coeff.shape[-1] <= self.order:\n",
    "            return torch.zeros_like(coeff[..., :1])  # shape (F, K, 1).\n",
    "\n",
    "        for _ in range(self.order):\n",
    "            degp1 = coeff.shape[-1]  # shape (F, K, dp1).\n",
    "            arange = torch.arange(1, degp1).to(coeff)  # shape (deg,).\n",
    "            coeff = coeff[..., 1:] * arange  # a_n x^n -> n a_n x^(n-1), with a_0 disappeared.\n",
    "\n",
    "        return coeff  # shape (F, K, dp1-ord).\n",
    "\n",
    "    # -------- unnecessary in this case, directly use inherited --------\n",
    "\n",
    "    # @property\n",
    "    # def fold_settings(self) -> Tuple[Any, ...]:\n",
    "    #     return super().fold_settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to register the mapping between the torch implementations with their symbolic conterparts. It should be simple to define in most cases.\n",
    "\n",
    "Note that each backend has its own registry instead of one large dict for everything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[cirkit/backend/torch/rules/parameters.py](../cirkit/backend/torch/rules/parameters.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirkit.backend.torch.compiler import TorchCompiler\n",
    "from cirkit.backend.torch.rules.parameters import DEFAULT_PARAMETER_COMPILATION_RULES\n",
    "\n",
    "\n",
    "def compile_polynomial_my_differential(\n",
    "    compiler: \"TorchCompiler\", p: PolynomialMyDifferential\n",
    ") -> TorchPolynomialMyDifferential:\n",
    "    return TorchPolynomialMyDifferential(*p.in_shapes, order=p.order)\n",
    "\n",
    "\n",
    "DEFAULT_PARAMETER_COMPILATION_RULES.update(\n",
    "    {PolynomialMyDifferential: compile_polynomial_my_differential}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline-level Convenience Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the above definition, the new operator should be available to use as `cirkit.symbolic.functional.my_differential`. However for convenience, we can also add the following to the `PipelineContext` class.\n",
    "\n",
    "Note that this must be done by modifying the class in the library instead of in the user code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[cirkit/pipeline.py](../cirkit/pipeline.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import AbstractContextManager\n",
    "\n",
    "import cirkit.symbolic.functional as SF\n",
    "from cirkit.backend.compiler import CompiledCircuit\n",
    "\n",
    "\n",
    "class PipelineContext(AbstractContextManager):\n",
    "    ...  # Any existing defs\n",
    "\n",
    "    def differentiate(self, cc: CompiledCircuit, *, order: int = 1) -> CompiledCircuit:\n",
    "        if not self._compiler.has_symbolic(cc):\n",
    "            raise ValueError(\"The given compiled circuit is not known in this pipeline\")\n",
    "        if order <= 0:\n",
    "            raise ValueError(\"The order of differentiation must be positive.\")\n",
    "        sc = self._compiler.get_symbolic_circuit(cc)\n",
    "        diff_sc = SF.differentiate(sc, registry=self._op_registry, order=order)\n",
    "        return self.compile(diff_sc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cirkit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
