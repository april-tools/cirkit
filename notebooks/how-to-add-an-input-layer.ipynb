{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How-To: Add an Input Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook introduces how to add a custom input layer. (To add sum/product layers, please open an issue to discuss).\n",
    "\n",
    "- For users: The code may be added to anywhere in your project, just make sure you have proper imports.\n",
    "- For developers: Please look at comments for each code block to decide where to add the code pieces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new layer requires a symbolic definition and the implementation(s) corresponding to the backend(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will illustrate the process with `MyPolynomialLayer` and its `torch` backend, which is a replicate of `PolynomialLayer` in the library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symbolic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the symbolic part, we will have to:\n",
    "- Add the definition of the layer;\n",
    "- Decide the operators supported by this layer;\n",
    "  - Identify the parameter operations required by the operators.\n",
    "\n",
    "All the above will not involve any actual tensors, just the configs and shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add as many operators as the layer supports, but for illustrative purposes, here we only illustrate with multiplication.\n",
    "\n",
    "For operators the layer does not support, just leave it out and it will be properly handled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For layer definition, we should include any configs it needs, along with any parameter it includes. The parameter can be constructed from an optionally provided parameter or a factory, or falls back to default which is a new parameter with normal initialization (init may be changed by additional args to `_make_param`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic set of methods to define for a layer consist of:\n",
    "- `__init__`: A must in most cases. Defines how to instantiate this layer.\n",
    "- `_{param}_shape`: One for each parameter, if any. Specifies the shape of the parameter.\n",
    "- `config`: Must-have if `__init__` accepts any args other than `scope`, `num_output_units`, `num_channels` and the params. Should be appended with any configs of the layer to `super().config`.\n",
    "- `params`: Must-have if the layer has any parameters. Includes all params in a dict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[cirkit/symbolic/layers.py](../cirkit/symbolic/layers.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "from cirkit.symbolic.layers import InputLayer\n",
    "from cirkit.symbolic.parameters import Parameter, ParameterFactory\n",
    "from cirkit.utils.scope import Scope\n",
    "\n",
    "\n",
    "class MyPolynomialLayer(InputLayer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        scope: Scope,\n",
    "        num_output_units: int,\n",
    "        num_channels: int,\n",
    "        *,\n",
    "        degree: int,\n",
    "        coeff: Parameter | None = None,\n",
    "        coeff_factory: ParameterFactory | None = None,\n",
    "    ):\n",
    "        if len(scope) != 1:\n",
    "            raise ValueError(\"The Polynomial layer encodes a univariate distribution\")\n",
    "        if num_channels != 1:\n",
    "            raise ValueError(\"The Polynomial layer encodes a univariate distribution\")\n",
    "        super().__init__(scope, num_output_units, num_channels)\n",
    "        self.degree = degree\n",
    "        coeff = self._make_param(coeff, coeff_factory, self._coeff_shape)\n",
    "        if coeff.shape != self._coeff_shape:\n",
    "            raise ValueError(f\"Expected parameter shape {self._coeff_shape}, found {coeff.shape}\")\n",
    "        self.coeff = coeff\n",
    "\n",
    "    @property\n",
    "    def _coeff_shape(self) -> tuple[int, ...]:\n",
    "        return self.num_output_units, self.degree + 1\n",
    "\n",
    "    @property\n",
    "    def config(self) -> dict[str, Any]:\n",
    "        return {**super().config, \"degree\": self.degree}\n",
    "\n",
    "    @property\n",
    "    def params(self) -> dict[str, Parameter]:\n",
    "        return {\"coeff\": self.coeff}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After deciding which operator(s) we want to support, we must define the parameter operations the operator(s) need(s).\n",
    "\n",
    "Since we are only looking at multiplication here, and the layer only has one parameter `coeff`, we only need to define one patameter operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As multiplication is a binary operator, we can inherit from `BinaryParameterOp` to make the best use of existing infrastructure. Alternatively, a more general `ParameterOp` class may be inherited.\n",
    "\n",
    "The mininum definition should include the `shape` property which defines the output shape of this parameter operation.\n",
    "\n",
    "Optionally, `__init__` can be redefined with customized instantiation behaviour, and `config` should include any additional args of `__init__`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[cirkit/symbolic/parameters.py](../cirkit/symbolic/parameters.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirkit.symbolic.parameters import BinaryParameterOp\n",
    "\n",
    "\n",
    "class MyPolynomialProduct(BinaryParameterOp):\n",
    "    @property\n",
    "    def shape(self) -> tuple[int, ...]:\n",
    "        return (\n",
    "            self.in_shapes[0][0] * self.in_shapes[1][0],  # dim Ko\n",
    "            self.in_shapes[0][1] + self.in_shapes[1][1] - 1,  # dim deg+1\n",
    "        )\n",
    "\n",
    "    # -------- unnecessary in this case, directly use inherited --------\n",
    "\n",
    "    # def __init__(self, in_shape1: tuple[int, ...], in_shape2: tuple[int, ...]):\n",
    "    #     super().__init__(in_shape1, in_shape2)\n",
    "\n",
    "    # @property\n",
    "    # def config(self) -> dict[str, Any]:\n",
    "    #     return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the layer and param op have been defined, we can then define how an operator act on the layer by defining a rule function and registering it to the rules registry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to share the underlying parameters across the operations, `param.ref()` should be passed to build the new parameter from the operators.\n",
    "\n",
    "And then, the resulting new layer (or can be layers, if needed) should be wrapped in a `CircuitBlock` for return."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[cirkit/symbolic/operators.py](../cirkit/symbolic/operators.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirkit.symbolic.circuit import CircuitBlock\n",
    "from cirkit.symbolic.layers import LayerOperator\n",
    "from cirkit.symbolic.operators import DEFAULT_OPERATOR_RULES\n",
    "from cirkit.symbolic.parameters import Parameter\n",
    "\n",
    "\n",
    "def multiply_mypolynomial_layers(sl1: MyPolynomialLayer, sl2: MyPolynomialLayer) -> CircuitBlock:\n",
    "    if sl1.scope != sl2.scope:\n",
    "        raise ValueError(\n",
    "            f\"Expected Polynomial layers to have the same scope,\"\n",
    "            f\" but found '{sl1.scope}' and '{sl2.scope}'\"\n",
    "        )\n",
    "    if sl1.num_channels != sl2.num_channels:\n",
    "        raise ValueError(\n",
    "            f\"Expected Polynomial layers to have the number of channels,\"\n",
    "            f\"but found '{sl1.num_channels}' and '{sl2.num_channels}'\"\n",
    "        )\n",
    "\n",
    "    coeff = Parameter.from_binary(\n",
    "        MyPolynomialProduct(sl1.coeff.shape, sl2.coeff.shape),\n",
    "        sl1.coeff.ref(),\n",
    "        sl2.coeff.ref(),\n",
    "    )\n",
    "\n",
    "    sl = MyPolynomialLayer(\n",
    "        sl1.scope,\n",
    "        sl1.num_output_units * sl2.num_output_units,\n",
    "        num_channels=sl1.num_channels,\n",
    "        degree=sl1.degree + sl2.degree,\n",
    "        coeff=coeff,\n",
    "    )\n",
    "    return CircuitBlock.from_layer(sl)\n",
    "\n",
    "\n",
    "DEFAULT_OPERATOR_RULES[LayerOperator.MULTIPLICATION].append(multiply_mypolynomial_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation with Backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the backend implementation, we will have to:\n",
    "- Implement the actual computation for the layer and operator(s);\n",
    "- Specify the rule that maps the implementation above with the symbolic layer/operator(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What has been provided in the symbolic part should has a corresponding implmentation with the backend, although the rules are actually what handles whether and how the symbolic representation is translated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `torch` Implementation - Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The layer will take in the actual `Tensor` for parameters and input, and should calculate the output `Tensor` in its `forward` function, as in the common practice of `torch`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic set of methods to implement for a layer consist of:\n",
    "- `__init__`: A must in most cases. Defines how to instantiate this layer. Note that `num_folds` is not specified here but handled automatically in the pipeline, while `num_variables` is implicitly provided as `scope_idx.shape[-1]`.\n",
    "- `_valid_{param}_shape`: Not requied but recommended. Checks if the parameter has correct shape and folding.\n",
    "- `fold_settings`: A must in most cases. Contains a shape that helps to decide which layers can be folded (same shape can be stacked). Should be appended with any extra shap (from non-default args as in `config`) that may affect folding, but no need to duplicate.\n",
    "- `config`: Must-have if `__init__` accepts any args other than `scope_idx`, `num_output_units`, `num_channels`, `semiring` and the params. Should append any configs of the layer to `super().config`.\n",
    "- `params`: Must-have if the layer has any parameters. Includes all params in a dict.\n",
    "- `forward`: Must-have in all cases. Defines the actually computation of this layer. It must follow the protocol:\n",
    "  - The input is the value that the circuit receives, sliced to the corresponding scope, with shape `(fold, channel, batch, variable)`. Note that for simplictiy layers always accept only one batch dimension, while multi batch dim is handled at the circuit level;\n",
    "  - The output is the value in the space defined by the specified semi-ring, with shape `(fold, batch, output_unit)`.\n",
    "- `integrate`: TODO: why we need it? what's the protocol?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[cirkit/backend/torch/layers/input.py](../cirkit/backend/torch/layers/input.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "\n",
    "from cirkit.backend.torch.layers.input import TorchInputLayer, polyval\n",
    "from cirkit.backend.torch.parameters.parameter import TorchParameter\n",
    "from cirkit.backend.torch.semiring import Semiring, SumProductSemiring\n",
    "\n",
    "# The same implementation of the imported polyval.\n",
    "\n",
    "# def polyval(coeff: Tensor, x: Tensor) -> Tensor:\n",
    "#     \"\"\"Evaluate polynomial given coefficients and point, with the shape for PolynomialLayer.\n",
    "\n",
    "#     Args:\n",
    "#         coeff (Tensor): The coefficients of the polynomial, shape (F, Ko, deg+1).\n",
    "#         x (Tensor): The point of the variable, shape (F, H, B, Ki), where H=Ki=1.\n",
    "\n",
    "#     Returns:\n",
    "#         Tensor: The value of the polymonial, shape (F, B, Ko).\n",
    "#     \"\"\"\n",
    "#     x = x.squeeze(dim=1)  # shape (F, H=1, B, Ki=1) -> (F, B, 1).\n",
    "#     y = x.new_zeros(*x.shape[:-1], coeff.shape[-2])  # shape (F, B, Ko).\n",
    "\n",
    "#     for a_n in reversed(coeff.unbind(dim=2)):  # Reverse iterator of the degree axis, shape (F, Ko).\n",
    "#         # a_n shape (F, Ko) -> (F, 1, Ko).\n",
    "#         y = torch.addcmul(a_n.unsqueeze(dim=1), x, y)  # y = a_n + x * y, by Horner's method.\n",
    "#     return y  # shape (F, B, Ko).\n",
    "\n",
    "\n",
    "class TorchMyPolynomialLayer(TorchInputLayer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        scope_idx: Tensor,\n",
    "        num_output_units: int,\n",
    "        *,\n",
    "        num_channels: int = 1,\n",
    "        degree: int,\n",
    "        coeff: TorchParameter,\n",
    "        semiring: Semiring | None = None,\n",
    "    ) -> None:\n",
    "        num_variables = scope_idx.shape[-1]\n",
    "        if num_variables != 1:\n",
    "            raise ValueError(\"The Polynomial layer encodes a univariate distribution\")\n",
    "        if num_channels != 1:\n",
    "            raise ValueError(\"The Polynomial layer encodes a univariate distribution\")\n",
    "        super().__init__(\n",
    "            scope_idx,\n",
    "            num_output_units,\n",
    "            num_channels=num_channels,\n",
    "            semiring=semiring,\n",
    "        )\n",
    "        self.degree = degree\n",
    "        if not self._valid_parameters_shape(coeff):\n",
    "            raise ValueError(\"The number of folds and shape of 'coeff' must match the layer's\")\n",
    "        self.coeff = coeff\n",
    "\n",
    "    def _valid_parameters_shape(self, p: TorchParameter) -> bool:\n",
    "        if p.num_folds != self.num_folds:\n",
    "            return False\n",
    "        return p.shape == (self.num_output_units, self.degree + 1)\n",
    "\n",
    "    @property\n",
    "    def fold_settings(self) -> tuple[Any, ...]:\n",
    "        return *super().fold_settings, self.degree + 1\n",
    "\n",
    "    @property\n",
    "    def config(self) -> dict[str, Any]:\n",
    "        return {**super().config, \"degree\": self.degree}\n",
    "\n",
    "    @property\n",
    "    def params(self) -> dict[str, TorchParameter]:\n",
    "        return {\"coeff\": self.coeff}\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Run forward pass.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): The input to this layer, shape (F, H=C, B, Ki=D).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The output of this layer, shape (F, B, Ko).\n",
    "        \"\"\"\n",
    "        coeff = self.coeff()  # shape (F, Ko, dp1)\n",
    "        return self.semiring.map_from(polyval(coeff, x), SumProductSemiring)\n",
    "\n",
    "    def integrate(self) -> Tensor:\n",
    "        raise TypeError(\"Cannot integrate a PolynomialLayer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `torch` Implementation - Operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `torch` version of operators also provides a `TorchBinaryParameterOp` for easier implementation, with `TorchParameterOp` for more customization.\n",
    "\n",
    "The minimal implementation can include only the `shape` of output parameter, and the `forward` that transforms the input parameter(s) to the output.\n",
    "\n",
    "And optionally, `__init__` may be defined to contain a customized instantiation, with `config` containing additional args and `fold_settings` contaning any additional shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[cirkit/backend/torch/parameters/nodes.py](../cirkit/backend/torch/parameters/nodes.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from cirkit.backend.torch.parameters.nodes import TorchBinaryParameterOp\n",
    "\n",
    "\n",
    "class TorchMyPolynomialProduct(TorchBinaryParameterOp):\n",
    "    @property\n",
    "    def shape(self) -> tuple[int, ...]:\n",
    "        return (\n",
    "            self.in_shapes[0][0] * self.in_shapes[1][0],  # dim K\n",
    "            self.in_shapes[0][1] + self.in_shapes[1][1] - 1,  # dim dp1\n",
    "        )\n",
    "\n",
    "    def forward(self, coeff1: Tensor, coeff2: Tensor) -> Tensor:\n",
    "        if coeff1.is_complex() or coeff2.is_complex():\n",
    "            fft = torch.fft.fft\n",
    "            ifft = torch.fft.ifft\n",
    "        else:\n",
    "            fft = torch.fft.rfft\n",
    "            ifft = torch.fft.irfft\n",
    "\n",
    "        degp1 = coeff1.shape[-1] + coeff2.shape[-1] - 1  # deg1p1 + deg2p1 - 1 = (deg1 + deg2) + 1.\n",
    "\n",
    "        spec1 = fft(coeff1, n=degp1, dim=-1)  # shape (F, K1, dp1).\n",
    "        spec2 = fft(coeff2, n=degp1, dim=-1)  # shape (F, K2, dp1).\n",
    "\n",
    "        # shape (F, K1, 1, dp1), (F, 1, K2, dp1) -> (F, K1, K2, dp1) -> (F, K1*K2, dp1).\n",
    "        spec = torch.flatten(\n",
    "            spec1.unsqueeze(dim=2) * spec2.unsqueeze(dim=1), start_dim=1, end_dim=2\n",
    "        )\n",
    "\n",
    "        return ifft(spec, n=degp1, dim=-1)  # shape (F, K1*K2, dp1).\n",
    "\n",
    "    # -------- unnecessary in this case, directly use inherited --------\n",
    "\n",
    "    # def __init__(\n",
    "    #     self, in_shape1: tuple[int, ...], in_shape2: tuple[int, ...], *, num_folds: int = 1\n",
    "    # ) -> None:\n",
    "    #     super().__init__(in_shape1, in_shape2, num_folds=num_folds)\n",
    "\n",
    "    # @property\n",
    "    # def fold_settings(self) -> tuple[Any, ...]:\n",
    "    #     return super().fold_settings\n",
    "\n",
    "    # @property\n",
    "    # def config(self) -> dict[str, Any]:\n",
    "    #     return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to register the mapping between the torch implementations with their symbolic conterparts. It should be simple to define in most cases.\n",
    "\n",
    "Note that each backend has its own registry instead of one large dict for everything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[cirkit/backend/torch/rules/layers.py](../cirkit/backend/torch/rules/layers.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirkit.backend.torch.compiler import TorchCompiler\n",
    "from cirkit.backend.torch.rules.layers import DEFAULT_LAYER_COMPILATION_RULES\n",
    "\n",
    "\n",
    "def compile_polynomial_layer(\n",
    "    compiler: TorchCompiler, sl: MyPolynomialLayer\n",
    ") -> TorchMyPolynomialLayer:\n",
    "    coeff = compiler.compile_parameter(sl.coeff)\n",
    "    return TorchMyPolynomialLayer(\n",
    "        torch.tensor(tuple(sl.scope)),\n",
    "        sl.num_output_units,\n",
    "        num_channels=sl.num_channels,\n",
    "        degree=sl.degree,\n",
    "        coeff=coeff,\n",
    "        semiring=compiler.semiring,\n",
    "    )\n",
    "\n",
    "\n",
    "DEFAULT_LAYER_COMPILATION_RULES.update({MyPolynomialLayer: compile_polynomial_layer})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[cirkit/backend/torch/rules/parameters.py](../cirkit/backend/torch/rules/parameters.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirkit.backend.torch.rules.parameters import DEFAULT_PARAMETER_COMPILATION_RULES\n",
    "\n",
    "\n",
    "def compile_polynomial_product(\n",
    "    compiler: TorchCompiler, p: MyPolynomialProduct\n",
    ") -> TorchMyPolynomialProduct:\n",
    "    return TorchMyPolynomialProduct(*p.in_shapes)\n",
    "\n",
    "\n",
    "DEFAULT_PARAMETER_COMPILATION_RULES.update({MyPolynomialProduct: compile_polynomial_product})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cirkit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
