{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1e6d6b9",
   "metadata": {},
   "source": [
    "# Learning and Evaluating a Probabilistic Circuit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bf2b04-012a-4d5f-8700-39e848f6b2c9",
   "metadata": {},
   "source": [
    "In this notebook, we instantiate, learn, and evaluate a probabilistic circuit using ```cirkit```. The probabilistic circuit we build estimates the distribution of MNIST images, which is then evaluated on unseen images, compute marginal probabilities, and sample new images. Here, we focus on the simplest experimental setting, where we want to instantiate a probabilistic circuit for MNIST images using some hyperparameters of our own choice, such as the type of the layers, their size and how to parameterize them. Then, we learn the parameters of the circuit and perform inference using PyTorch.\n",
    "\n",
    "A key feature of ```cirkit``` is the _symbolic circuit representation_, which allows us to abstract away from the underlying implementation choices. In the next section, we introduce this symbolic representation and show how to construct a symbolic circuit whose structure and parameterization is tailored for image data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66900897-5d65-4136-8399-f997c3665a38",
   "metadata": {},
   "source": [
    "## Constructing the Symbolic Circuit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6468f6-a1e5-46f8-92d4-d1eb6fa5ba19",
   "metadata": {},
   "source": [
    "The **symbolic circuit** is a symbolic abstraction of a tensorized circuit, i.e., a circuit consisting of sum/product/input layers, each grouping several sum/product/input units, respectively. This symbolic representation stores the connections between the layers, the number of units in each layer, and useful metadata about the parameters, such as their shape and parameterization choices. Note that a symbolic circuit does not allocate parameters and cannot be used for learning or inference. By _compiling a symbolic circuit_ using PyTorch, we will later recover a probabilistic circuit that can be learned or be used for inference purposes.\n",
    "\n",
    "In ```cirkit.templates```, we provide several templates that can be used to construct symbolic circuits of different structures. In this notebook, we use a high-level template to build a symbolic circuit specifically for image data. To do so, we need to specify some arguments that will possibly yield different architectures and parameterizations. That is, we specify the shape of the images, and select one of the region graphs that exploits the closeness of patches of pixels, such as the _QuadGraph_ region graph.\n",
    "See the [region-graph-and-parameterisations.ipynb](region-graph-and-parameterisations.ipynb) notebook for more details about region graphs. Moreover, we select the type of input and inner layers, the number of units within them, and how to parameterize the sum layers. See comments in the code below for more details about each argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52c88f38-1552-4d13-b62d-931493c07c69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T14:18:53.113003Z",
     "start_time": "2024-10-09T14:18:52.822643Z"
    }
   },
   "outputs": [],
   "source": [
    "from cirkit.templates import circuit_templates\n",
    "\n",
    "symbolic_circuit = circuit_templates.image_data(\n",
    "    (1, 28, 28),                # The shape of MNIST image, i.e., (num_channels, image_height, image_width)\n",
    "    region_graph='quad-graph',  # Select the structure of the circuit to follow the QuadGraph region graph\n",
    "    input_layer='categorical',  # Use Categorical distributions for the pixel values (0-255) as input layers\n",
    "    num_input_units=64,         # Each input layer consists of 64 Categorical input units\n",
    "    sum_product_layer='cp',     # Use CP sum-product layers, i.e., alternate dense layers with Hadamard product layers\n",
    "    num_sum_units=64,           # Each dense sum layer consists of 64 sum units\n",
    "    sum_weight_param=circuit_templates.Parameterization(\n",
    "        activation='softmax',   # Parameterize the sum weights by using a softmax activation\n",
    "        initialization='normal' # Initialize the sum weights by sampling from a standard normal distribution\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8c6e7c-ad9f-4dd2-ab76-602e191d197b",
   "metadata": {},
   "source": [
    "We can query some information regarding the symbolic circuit, such as the number of variables and channels it is defined on, and which structural properties it does satisfy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23570bfd-a64e-4e19-ba4c-30e489e9d08d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T14:18:53.181658Z",
     "start_time": "2024-10-09T14:18:53.147512Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of variables: 784\n",
      "Number of channels per variable: 1\n",
      "\n",
      "Structural properties:\n",
      "  - Smoothness: True\n",
      "  - Decomposability: True\n",
      "  - Structured-decomposability: False\n"
     ]
    }
   ],
   "source": [
    "# Print some information\n",
    "print(f'Number of variables: {symbolic_circuit.num_variables}')\n",
    "print(f'Number of channels per variable: {symbolic_circuit.num_channels}')\n",
    "print()\n",
    "\n",
    "# Print which structural properties the circuit satisfies\n",
    "print(f'Structural properties:')\n",
    "print(f'  - Smoothness: {symbolic_circuit.is_smooth}')\n",
    "print(f'  - Decomposability: {symbolic_circuit.is_decomposable}')\n",
    "print(f'  - Structured-decomposability: {symbolic_circuit.is_structured_decomposable}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67398c38",
   "metadata": {},
   "source": [
    "## Compiling the Symbolic Circuit with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba547d8",
   "metadata": {},
   "source": [
    "After we have built our symbolic circuit, it is necessary to **compile** it in order to learn the parameters and perform probabilistic inference. By default, cirkit compiles symbolic circuits using PyTorch 2+. More precisely, by compiling a symbolic circuit, we retrieve a tensorized circuit that specializes [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html), thus being very similar to a neural network written in PyTorch. First, we set some random seeds and set the torch device we will use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9ea4a4a-649d-462f-bcfd-20a12bd8a052",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T14:18:54.983524Z",
     "start_time": "2024-10-09T14:18:53.330911Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Set some seeds\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# Set the torch device to use\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f7a3f9-1c64-4117-81f0-8766ccbd3179",
   "metadata": {},
   "source": [
    "Next, we import the ```compile``` function from the ```cirkit.pipeline``` module and compile our symbolic circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af58c11e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T14:18:57.132288Z",
     "start_time": "2024-10-09T14:18:54.991124Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.06 s, sys: 323 ms, total: 3.39 s\n",
      "Wall time: 3.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from cirkit.pipeline import compile\n",
    "circuit = compile(symbolic_circuit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed82999-59f8-411d-8447-c486991293d7",
   "metadata": {},
   "source": [
    "Note that the compilation procedure took about three seconds for a circuit with >5700 layers and ~25M parameters, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0a1892e-4a65-4759-bb3a-ccbe6f5e515c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T14:18:57.152272Z",
     "start_time": "2024-10-09T14:18:57.148418Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 5725\n",
      "Number of learnable parameters: 25657730\n"
     ]
    }
   ],
   "source": [
    "# Print some statistics\n",
    "num_layers = len(list(symbolic_circuit.layers))\n",
    "print(f\"Number of layers: {num_layers}\")\n",
    "num_parameters = sum(p.numel() for p in circuit.parameters())\n",
    "print(f\"Number of learnable parameters: {num_parameters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beee1f04",
   "metadata": {},
   "source": [
    "## Learning a Probabilistic Circuit using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bf4858",
   "metadata": {},
   "source": [
    "Learning the probabilistic circuit we have compiled above can be done in the same way as any other neural network written using PyTorch. In this notebook, we learn the parameters of the probabilistic circuit as to estimate the distribution of MNIST images. Therefore, below we load the MNIST data set using [torchvision](https://pytorch.org/vision/stable/index.html), and we instantiate the training and testing data loaders. In addition, we select one of the many optimizers implemented in PyTorch, such as [Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02854883",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T14:18:58.418158Z",
     "start_time": "2024-10-09T14:18:57.224536Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "# Load the MNIST data set and data loaders\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # Flatten the images and set pixel values in the [0-255] range\n",
    "    transforms.Lambda(lambda x: (255 * x.view(-1)).long())\n",
    "])\n",
    "data_train = datasets.MNIST('datasets', train=True, download=True, transform=transform)\n",
    "data_test = datasets.MNIST('datasets', train=False, download=True, transform=transform)\n",
    "\n",
    "# Instantiate the training and testing data loaders\n",
    "train_dataloader = DataLoader(data_train, shuffle=True, batch_size=256)\n",
    "test_dataloader = DataLoader(data_test, shuffle=False, batch_size=256)\n",
    "\n",
    "# Initialize a torch optimizer of your choice,\n",
    "#  e.g., Adam, by passing the parameters of the circuit\n",
    "optimizer = optim.Adam(circuit.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e08fe19-607d-42dc-8ec7-3ab1b6110274",
   "metadata": {},
   "source": [
    "Finally, we write a classical PyTorch training loop that iterates over batches of MNIST images for some epochs, and optimizes the parameters of the circuit by minimizing the negated average log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f28e9c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T14:21:45.586683Z",
     "start_time": "2024-10-09T14:18:58.426109Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 200: Average NLL: 2492.162\n",
      "Step 400: Average NLL: 895.924\n",
      "Step 600: Average NLL: 785.733\n",
      "Step 800: Average NLL: 749.979\n",
      "Step 1000: Average NLL: 729.827\n",
      "Step 1200: Average NLL: 716.521\n",
      "Step 1400: Average NLL: 707.093\n",
      "Step 1600: Average NLL: 698.421\n",
      "Step 1800: Average NLL: 693.506\n",
      "Step 2000: Average NLL: 687.055\n",
      "Step 2200: Average NLL: 684.551\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "step_idx = 0\n",
    "running_loss = 0.0\n",
    "running_samples = 0\n",
    "\n",
    "# Move the circuit to chosen device\n",
    "circuit = circuit.to(device)\n",
    "\n",
    "for epoch_idx in range(num_epochs):\n",
    "    for i, (batch, _) in enumerate(train_dataloader):\n",
    "        # The circuit expects an input of shape (batch_dim, num_channels, num_variables),\n",
    "        # so we unsqueeze a dimension for the channel.\n",
    "        batch = batch.to(device).unsqueeze(dim=1)\n",
    "\n",
    "        # Compute the log-likelihoods of the batch, by evaluating the circuit\n",
    "        log_likelihoods = circuit(batch)\n",
    "\n",
    "        # We take the negated average log-likelihood as loss\n",
    "        loss = -torch.mean(log_likelihoods)\n",
    "        loss.backward()\n",
    "        # Update the parameters of the circuits, as any other model in PyTorch\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        running_loss += loss.detach() * len(batch)\n",
    "        running_samples += len(batch)\n",
    "        step_idx += 1\n",
    "        if step_idx % 200 == 0:\n",
    "            average_nll = running_loss / running_samples\n",
    "            print(f\"Step {step_idx}: Average NLL: {average_nll:.3f}\")\n",
    "            running_loss = 0.0\n",
    "            running_samples = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50efae6b",
   "metadata": {},
   "source": [
    "Similarly, we evaluate our probabilistic circuit on the test data by computing the average log-likelihood and bits per dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e66bd8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T14:21:48.557821Z",
     "start_time": "2024-10-09T14:21:45.715485Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test LL: -683.001\n",
      "Bits per dimension: 1.257\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    test_lls = 0.0\n",
    "\n",
    "    for batch, _ in test_dataloader:\n",
    "        # The circuit expects an input of shape (batch_dim, num_channels, num_variables),\n",
    "        # so we unsqueeze a dimension for the channel.\n",
    "        batch = batch.to(device).unsqueeze(dim=1)\n",
    "\n",
    "        # Compute the log-likelihoods of the batch\n",
    "        log_likelihoods = circuit(batch)\n",
    "\n",
    "        # Accumulate the log-likelihoods\n",
    "        test_lls += log_likelihoods.sum().item()\n",
    "\n",
    "    # Compute average test log-likelihood and bits per dimension\n",
    "    average_ll = test_lls / len(data_test)\n",
    "    bpd = -average_ll / (28 * 28 * np.log(2.0))\n",
    "    print(f\"Average test LL: {average_ll:.3f}\")\n",
    "    print(f\"Bits per dimension: {bpd:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4033c3f8-424e-4859-aab2-c3d5cdbf5d41",
   "metadata": {},
   "source": [
    "## Performing Inference: computing marginal probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573c533c-5f2a-4793-8b0a-77e8c300cb96",
   "metadata": {},
   "source": [
    "Probabilistic circuits (PCs) like the one we have learned can be used to answer to a number of probabilistic queries exactly and efficiently. This is very different from other generative models where probabilities can only be approximated at best, such as variational auto-encoders.\n",
    "\n",
    "Here, we show a simple example of performing probabilistic inference on the learned PC: computing marginal probabilities. More specifically, we show how to compute the marginal probability of the pixels on the right half side of the image, i.e.,\n",
    "$$\n",
    "p(\\mathbf{x}_{\\text{R}}) = \\sum_{\\mathbf{x}_{\\text{L}}\\in[0,255]^{28\\times 14}} p(\\mathbf{x}_{\\text{R}}, \\mathbf{x}_{\\text{L}}),\n",
    "$$\n",
    "where $\\mathbf{x}_{\\text{R}}$ (resp. $\\mathbf{x}_{\\text{L}}$) denotes an assignment to the pixels on the right half side (resp. left half side) of the MNIST image, and the joint distribution $p(\\mathbf{x}_{\\text{R}}, \\mathbf{x}_{\\text{L}})$ is modelled by the probabilistic circuit we have learned above.\n",
    "\n",
    "In order to compute a marginal probability, we need to _integrate the probabilistic circuit over a variables' subset_. Note that, with a slight abuse of terminology, we will refer to \"integrate\" to indicate also the case of finite discrete summations (e.g., as in the marginal probability above). In practice, we leverage the ```IntegrateQuery``` object provided by the PyTorch backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20ac5a71-895a-40d0-98a7-17044fe8e714",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T14:21:48.627925Z",
     "start_time": "2024-10-09T14:21:48.623516Z"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from cirkit.utils.scope import Scope\n",
    "from cirkit.backend.torch.queries import IntegrateQuery\n",
    "\n",
    "# Build the scope to marginalize, i.e., the set integer ids of the variables that will be integrated over\n",
    "vars_to_marginalize = Scope(j * 28 + i for j, i in itertools.product(range(28), range(28)) if i < 14)\n",
    "\n",
    "# Instantiate the integrate query handler\n",
    "marginal_query = IntegrateQuery(circuit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c570b26-7e97-4927-8449-99593f6e76c8",
   "metadata": {},
   "source": [
    "The ```marginal_query``` object can then be used to compute marginal probabilities. We show an example below, where we compute the average marginal probability of right half side patches of test images in MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b0bd29b-021b-4f21-8610-139faea76a39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T14:21:51.577957Z",
     "start_time": "2024-10-09T14:21:48.680205Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test marginal LL: -378.368\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    test_marginal_lls = 0.0\n",
    "\n",
    "    for batch, _ in test_dataloader:\n",
    "        # The marginal circuit still expects an input of shape (batch_dim, num_channels, num_variables),\n",
    "        # so we unsqueeze a dimension for the channel. However, only some of the variables will be \n",
    "        # effectively used in the feed-forward evaluation.\n",
    "        batch = batch.to(device).unsqueeze(dim=1)\n",
    "\n",
    "        # Compute the marginal log-likelihoods of the batch\n",
    "        marginal_log_likelihoods = marginal_query(batch, integrate_vars=vars_to_marginalize)\n",
    "\n",
    "        # Accumulate the marginal log-likelihoods\n",
    "        test_marginal_lls += marginal_log_likelihoods.sum().item()\n",
    "    \n",
    "    # Compute average test marginal log-likelihood\n",
    "    average_marginal_ll = test_marginal_lls / len(data_test)\n",
    "    print(f\"Average test marginal LL: {average_marginal_ll:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
