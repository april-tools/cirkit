{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1e6d6b9",
   "metadata": {},
   "source": [
    "# Learning and Evaluating a Probabilistic Circuit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bf2b04-012a-4d5f-8700-39e848f6b2c9",
   "metadata": {},
   "source": [
    "In this notebook, we instantiate, learn, and evaluate a probabilistic circuit using ```cirkit```. The probabilistic circuit we build estimates the distribution of MNIST images, which is then evaluated on unseen images and used to compute marginal probabilities. Here, we focus on the simplest experimental setting, where we want to instantiate a probabilistic circuit for MNIST images using some hyperparameters of our own choice, such as the type of the layers, their size and how to parameterize them. Then, we learn the parameters of the circuit and perform inference using PyTorch.\n",
    "\n",
    "A key feature of ```cirkit``` is the _symbolic circuit representation_, which allows us to abstract away from the underlying implementation choices. In the next section, we introduce this symbolic representation and show how to construct a symbolic circuit whose structure and parameterization is tailored for image data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66900897-5d65-4136-8399-f997c3665a38",
   "metadata": {},
   "source": [
    "## Constructing the Symbolic Circuit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6468f6-a1e5-46f8-92d4-d1eb6fa5ba19",
   "metadata": {},
   "source": [
    "The **symbolic circuit** is a symbolic abstraction of a tensorized circuit, i.e., a circuit consisting of sum/product/input layers, each grouping several sum/product/input units, respectively. This symbolic representation stores the connections between the layers, the number of units in each layer, and useful metadata about the parameters, such as their shape and parameterization choices. Note that a symbolic circuit does not allocate parameters and cannot be used for learning or inference. By _compiling a symbolic circuit_ using PyTorch, we will later recover a probabilistic circuit that can be learned or be used for inference purposes.\n",
    "\n",
    "In ```cirkit.templates```, we provide several templates that can be used to construct symbolic circuits of different structures. In this notebook, we use a high-level template to build a symbolic circuit specifically for image data. To do so, we need to specify some arguments that will possibly yield different architectures and parameterizations. That is, we specify the shape of the images, and select one of the region graphs that exploits the closeness of patches of pixels, such as the _QuadGraph_ region graph (see (TODO: ref) for more details). Moreover, we select the type of input and inner layers, the number of units within them, and how to parameterize the sum layers. See comments in the code below for more details about each argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52c88f38-1552-4d13-b62d-931493c07c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirkit.templates import circuit_templates\n",
    "\n",
    "symbolic_circuit = circuit_templates.image_data(\n",
    "    (1, 28, 28),                # The shape of MNIST image, i.e., (num_channels, image_height, image_width)\n",
    "    region_graph='quad-graph',  # Select the structure of the circuit to follow the QuadGraph region graph\n",
    "    input_layer='categorical',  # Use Categorical distributions for the pixel values (0-255) as input layers\n",
    "    num_input_units=64,         # Each input layer consists of 64 Categorical input units\n",
    "    sum_product_layer='cp',     # Use CP sum-product layers, i.e., alternate dense sum layers and hadamard product layers\n",
    "    num_sum_units=64,           # Each dense sum layer consists of 64 sum units\n",
    "    sum_weight_param='softmax'  # Parameterize the weights of dense sum layers with 'softmax'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8c6e7c-ad9f-4dd2-ab76-602e191d197b",
   "metadata": {},
   "source": [
    "We can query some information regarding the symbolic circuit, such as the number of variables and channels it is defined on, and which structural properties it does satisfy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23570bfd-a64e-4e19-ba4c-30e489e9d08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of variables: 784\n",
      "Number of channels per variable: 1\n",
      "\n",
      "Structural properties:\n",
      "  - Smoothness: True\n",
      "  - Decomposability: True\n",
      "  - Structured-decomposability: False\n"
     ]
    }
   ],
   "source": [
    "# Print some information\n",
    "print(f'Number of variables: {symbolic_circuit.num_variables}')\n",
    "print(f'Number of channels per variable: {symbolic_circuit.num_channels}')\n",
    "print()\n",
    "\n",
    "# Print which structural properties the circuit satisfies\n",
    "print(f'Structural properties:')\n",
    "print(f'  - Smoothness: {symbolic_circuit.is_smooth}')\n",
    "print(f'  - Decomposability: {symbolic_circuit.is_decomposable}')\n",
    "print(f'  - Structured-decomposability: {symbolic_circuit.is_structured_decomposable}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67398c38",
   "metadata": {},
   "source": [
    "## Compiling the Symbolic Circuit with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba547d8",
   "metadata": {},
   "source": [
    "After we have built our symbolic circuit, it is necessary to **compile** it in order to learn the parameters and perform probabilistic inference. By default, cirkit compiles symbolic circuits using PyTorch 2+. More precisely, by compiling a symbolic circuit, we retrieve a tensorized circuit that specializes [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html), thus being very similar to a neural network written in PyTorch. First, we set some random seeds and set the torch device we will use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9ea4a4a-649d-462f-bcfd-20a12bd8a052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Set some seeds\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# Set the torch device to use\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f7a3f9-1c64-4117-81f0-8766ccbd3179",
   "metadata": {},
   "source": [
    "Next, we import the ```compile``` function from the ```cirkit.pipeline``` module and compile our symbolic circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af58c11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.51 s, sys: 223 ms, total: 2.73 s\n",
      "Wall time: 2.65 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from cirkit.pipeline import compile\n",
    "circuit = compile(symbolic_circuit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed82999-59f8-411d-8447-c486991293d7",
   "metadata": {},
   "source": [
    "Note that the compilation procedure took about three seconds for a circuit with >5700 layers and ~25M parameters, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0a1892e-4a65-4759-bb3a-ccbe6f5e515c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 5723\n",
      "Number of learnable parameters: 25641474\n"
     ]
    }
   ],
   "source": [
    "# Print some statistics\n",
    "num_layers = len(list(symbolic_circuit.layers))\n",
    "print(f\"Number of layers: {num_layers}\")\n",
    "num_parameters = sum(p.numel() for p in circuit.parameters())\n",
    "print(f\"Number of learnable parameters: {num_parameters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beee1f04",
   "metadata": {},
   "source": [
    "## Learning a Probabilistic Circuit using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bf4858",
   "metadata": {},
   "source": [
    "Learning the probabilistic circuit we have compiled above can be done in the same way as any other neural network written using PyTorch. In this notebook, we learn the parameters of the probabilistic circuit as to estimate the distribution of MNIST images. Therefore, below we load the MNIST data set using [torchvision](https://pytorch.org/vision/stable/index.html), and we instantiate the training and testing data loaders. In addition, we select one of the many optimizers implemented in PyTorch, such as [Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02854883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "# Load the MNIST data set and data loaders\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # Flatten the images and set pixel values in the [0-255] range\n",
    "    transforms.Lambda(lambda x: (255 * x.view(-1)).long())\n",
    "])\n",
    "data_train = datasets.MNIST('datasets', train=True, download=True, transform=transform)\n",
    "data_test = datasets.MNIST('datasets', train=False, download=True, transform=transform)\n",
    "\n",
    "# Instantiate the training and testing data loaders\n",
    "train_dataloader = DataLoader(data_train, shuffle=True, batch_size=256)\n",
    "test_dataloader = DataLoader(data_test, shuffle=False, batch_size=256)\n",
    "\n",
    "# Initialize a torch optimizer of your choice,\n",
    "#  e.g., Adam, by passing the parameters of the circuit\n",
    "optimizer = optim.Adam(circuit.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e08fe19-607d-42dc-8ec7-3ab1b6110274",
   "metadata": {},
   "source": [
    "Finally, we write a classical PyTorch training loop that iterates over batches of MNIST images for some epochs, and optimizes the parameters of the circuit by minimizing the negated average log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f28e9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100: Average NLL: 3413.503\n",
      "Step 200: Average NLL: 1570.938\n",
      "Step 300: Average NLL: 943.364\n",
      "Step 400: Average NLL: 844.029\n",
      "Step 500: Average NLL: 792.305\n",
      "Step 600: Average NLL: 771.743\n",
      "Step 700: Average NLL: 757.542\n",
      "Step 800: Average NLL: 737.154\n",
      "Step 900: Average NLL: 734.906\n",
      "Step 1000: Average NLL: 719.278\n",
      "Step 1100: Average NLL: 718.698\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "step_idx = 0\n",
    "running_loss = 0.0\n",
    "\n",
    "# Move the circuit to chosen device\n",
    "circuit = circuit.to(device)\n",
    "\n",
    "for epoch_idx in range(num_epochs):\n",
    "    for i, (batch, _) in enumerate(train_dataloader):\n",
    "        # The circuit expects an input of shape (batch_dim, num_channels, num_variables),\n",
    "        # so we unsqueeze a dimension for the channel.\n",
    "        batch = batch.to(device).unsqueeze(dim=1)\n",
    "\n",
    "        # Compute the log-likelihoods of the batch, by evaluating the circuit\n",
    "        log_likelihoods = circuit(batch)\n",
    "\n",
    "        # We take the negated average log-likelihood as loss\n",
    "        loss = -torch.mean(log_likelihoods)\n",
    "        loss.backward()\n",
    "        # Update the parameters of the circuits, as any other model in PyTorch\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        running_loss += loss.detach() * len(batch)\n",
    "        step_idx += 1\n",
    "        if step_idx % 100 == 0:\n",
    "            print(f\"Step {step_idx}: Average NLL: {running_loss / (100 * len(batch)):.3f}\")\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50efae6b",
   "metadata": {},
   "source": [
    "Similarly, we evaluate our probabilistic circuit on the test data by computing the average log-likelihood and bits per dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e66bd8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test LL: -711.308\n",
      "Bits per dimension: 1.309\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    test_lls = 0.0\n",
    "\n",
    "    for batch, _ in test_dataloader:\n",
    "        # The circuit expects an input of shape (batch_dim, num_channels, num_variables),\n",
    "        # so we unsqueeze a dimension for the channel.\n",
    "        batch = batch.to(device).unsqueeze(dim=1)\n",
    "\n",
    "        # Compute the log-likelihoods of the batch\n",
    "        log_likelihoods = circuit(batch)\n",
    "\n",
    "        # Accumulate the log-likelihoods\n",
    "        test_lls += log_likelihoods.sum().item()\n",
    "\n",
    "    # Compute average test log-likelihood and bits per dimension\n",
    "    average_ll = test_lls / len(data_test)\n",
    "    bpd = -average_ll / (28 * 28 * np.log(2.0))\n",
    "    print(f\"Average test LL: {average_ll:.3f}\")\n",
    "    print(f\"Bits per dimension: {bpd:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4033c3f8-424e-4859-aab2-c3d5cdbf5d41",
   "metadata": {},
   "source": [
    "## Performing Inference: computing marginal probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573c533c-5f2a-4793-8b0a-77e8c300cb96",
   "metadata": {},
   "source": [
    "We conclude the notebook by showing a simple example of performing probabilistic inference on the learned probabilistic circuit: computing marginal probabilities. More specifically, we show how to compute the marginal probability of the pixels on the right half side of the image, i.e.,\n",
    "$$\n",
    "p(\\mathbf{x}_{\\text{R}}) = \\sum_{\\mathbf{x}_{\\text{L}}\\in[0,255]^{28\\times 14}} p(\\mathbf{x}_{\\text{R}}, \\mathbf{x}_{\\text{L}}),\n",
    "$$\n",
    "where $\\mathbf{x}_{\\text{R}}$ (resp. $\\mathbf{x}_{\\text{L}}$) denotes an assignment to the pixels on the right half side (resp. left half side) of the MNIST image, and the joint distribution $p(\\mathbf{x}_{\\text{R}}, \\mathbf{x}_{\\text{L}})$ is modelled by the probabilistic circuit we have learned above.\n",
    "\n",
    "In order to compute a marginal probability, we need to _integrate the probabilistic circuit over a variables' subset_. Note that, with a slight abuse of terminology, we will refer to \"integrate\" to indicate also the case of finite discrete summations (e.g., as in the marginal probability above). In practice, we leverage the ```integrate``` operation provided by the ```cirkit.pipeline``` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20ac5a71-895a-40d0-98a7-17044fe8e714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from cirkit.utils.scope import Scope\n",
    "from cirkit.pipeline import integrate\n",
    "\n",
    "# Build the scope to marginalize, i.e., the set integer ids of the variables that will be integrated over\n",
    "scope_to_marginalize = Scope(j * 28 + i for j, i in itertools.product(range(28), range(28)) if i < 14)\n",
    "\n",
    "# Integrate the circuit and retrieve another circuit over the remaining set of variables\n",
    "marginal_circuit = integrate(circuit, scope=scope_to_marginalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c570b26-7e97-4927-8449-99593f6e76c8",
   "metadata": {},
   "source": [
    "The result of the ```integrate``` operation shown in the code above is another circuit in PyTorch that is defined over the remaining set of variables, i.e., those variables that have _not_ been marginalized out. Furthermore, the ```marginal_circuit``` obtained in the code above will share the same parameters of ```circuit```. Therefore, evaluating ```marginal_circuit``` on some input corresponds to compute the marginal probability $p(\\mathbf{x}_{\\text{R}})$. We show an example below, where we compute the average marginal probability of right half side patches of test images in MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b0bd29b-021b-4f21-8610-139faea76a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test marginal LL: -392.300\n"
     ]
    }
   ],
   "source": [
    "# Move the marginal circuit to device\n",
    "marginal_circuit.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_marginal_lls = 0.0\n",
    "\n",
    "    for batch, _ in test_dataloader:\n",
    "        # The marginal circuit still expects an input of shape (batch_dim, num_channels, num_variables),\n",
    "        # so we unsqueeze a dimension for the channel. However, only some of the variables will be \n",
    "        # effectively used in the feed-forward evaluation.\n",
    "        batch = batch.to(device).unsqueeze(dim=1)\n",
    "\n",
    "        # Compute the marginal log-likelihoods of the batch\n",
    "        marginal_log_likelihoods = marginal_circuit(batch)\n",
    "\n",
    "        # Accumulate the marginal log-likelihoods\n",
    "        test_marginal_lls += marginal_log_likelihoods.sum().item()\n",
    "    \n",
    "    # Compute average test marginal log-likelihood\n",
    "    average_marginal_ll = test_marginal_lls / len(data_test)\n",
    "    print(f\"Average test marginal LL: {average_marginal_ll:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfff6092-16e0-4d55-8622-7c30f1ac9227",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
