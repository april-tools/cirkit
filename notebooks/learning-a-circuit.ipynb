{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1e6d6b9",
   "metadata": {},
   "source": [
    "# Learning and Evaluating a Probabilistic Circuit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bf2b04-012a-4d5f-8700-39e848f6b2c9",
   "metadata": {},
   "source": [
    "In [a previous notebook](../learning-a-gaussian-mixture-model) we show how to construct and learn a circuit with ```cirkit```, manually building the symbolic circuit by hand. However, this can get quite cumbersome once we have enough input features.\n",
    "\n",
    "In this notebook, we will see how to **leverage built-in functions** from ```cirkit``` to easily build a deep circuit with (potentially) million of parameters. In particular, we will fit such a model on the MNIST dataset, and see how we can evaluate it on unseen images. \n",
    "\n",
    "Next, we show how to construct a symbolic circuit whose structure and parameterization is tailored for images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66900897-5d65-4136-8399-f997c3665a38",
   "metadata": {},
   "source": [
    "## Constructing the Symbolic Circuit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6468f6-a1e5-46f8-92d4-d1eb6fa5ba19",
   "metadata": {},
   "source": [
    "The **symbolic circuit** is a symbolic abstraction of a tensorized circuit. This representation tracks the layer connections, number of units per layer, and other useful metadata about the parameters, such as their shape and parameterization choices. \n",
    "\n",
    "We provide in ```cirkit.templates``` helper functions to build symbolic circuits with different structures. We will use one tailored for image data, providing some arguments that determine the shape and form of the circuit. \n",
    "\n",
    "For example, we choose _QuadGraph_ as our region graph which exploits the closeness of patches of pixels. See the [notebook on region graphs and sum product layers](../region-graphs-and-parametrisation) for more details about region graphs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52c88f38-1552-4d13-b62d-931493c07c69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T14:18:53.113003Z",
     "start_time": "2024-10-09T14:18:52.822643Z"
    }
   },
   "outputs": [],
   "source": [
    "from cirkit.templates import data_modalities, utils\n",
    "\n",
    "symbolic_circuit = data_modalities.image_data(\n",
    "    (1, 28, 28),                # The shape of MNIST image, i.e., (num_channels, image_height, image_width)\n",
    "    region_graph='quad-graph',  # Select the structure of the circuit to follow the QuadGraph region graph\n",
    "    input_layer='categorical',  # Use Categorical distributions for the pixel values (0-255) as input layers\n",
    "    num_input_units=64,         # Each input layer consists of 64 Categorical input units\n",
    "    sum_product_layer='cp',     # Use CP sum-product layers, i.e., alternate dense layers with Hadamard product layers\n",
    "    num_sum_units=64,           # Each dense sum layer consists of 64 sum units\n",
    "    sum_weight_param=utils.Parameterization(\n",
    "        activation='softmax',   # Parameterize the sum weights by using a softmax activation\n",
    "        initialization='normal' # Initialize the sum weights by sampling from a standard normal distribution\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8c6e7c-ad9f-4dd2-ab76-602e191d197b",
   "metadata": {},
   "source": [
    "We can query some information regarding the symbolic circuit, such as the number of variables it is defined on, and which structural properties it does satisfy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23570bfd-a64e-4e19-ba4c-30e489e9d08d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T14:18:53.181658Z",
     "start_time": "2024-10-09T14:18:53.147512Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of variables: 784\n",
      "\n",
      "Structural properties:\n",
      "  - Smoothness: True\n",
      "  - Decomposability: True\n",
      "  - Structured-decomposability: False\n"
     ]
    }
   ],
   "source": [
    "# Print some information\n",
    "print(f'Number of variables: {symbolic_circuit.num_variables}')\n",
    "print()\n",
    "\n",
    "# Print which structural properties the circuit satisfies\n",
    "print(f'Structural properties:')\n",
    "print(f'  - Smoothness: {symbolic_circuit.is_smooth}')\n",
    "print(f'  - Decomposability: {symbolic_circuit.is_decomposable}')\n",
    "print(f'  - Structured-decomposability: {symbolic_circuit.is_structured_decomposable}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67398c38",
   "metadata": {},
   "source": [
    "## Compiling the Symbolic Circuit with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c10766b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Set some seeds\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# Set the torch device to use\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba547d8",
   "metadata": {},
   "source": [
    "Note that *a symbolic circuit does not allocate parameters and cannot be used for learning or inference*, we need to **compile** the symbolic circuit. \n",
    "\n",
    "By default, ```cirkit``` compiles symbolic circuits using PyTorch 2+. Namely, it yields a regular [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) representing a tensorized circuit. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f7a3f9-1c64-4117-81f0-8766ccbd3179",
   "metadata": {},
   "source": [
    "Next, we import and use the ```compile``` function from ```cirkit.pipeline```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af58c11e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T14:18:57.132288Z",
     "start_time": "2024-10-09T14:18:54.991124Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.31 s, sys: 161 ms, total: 2.47 s\n",
      "Wall time: 2.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from cirkit.pipeline import compile\n",
    "circuit = compile(symbolic_circuit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed82999-59f8-411d-8447-c486991293d7",
   "metadata": {},
   "source": [
    "Note that the compilation took a couple seconds for a circuit with >5700 layers and ~25M parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0a1892e-4a65-4759-bb3a-ccbe6f5e515c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T14:18:57.152272Z",
     "start_time": "2024-10-09T14:18:57.148418Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 5725\n",
      "Number of learnable parameters: 25657730\n"
     ]
    }
   ],
   "source": [
    "# Print some statistics\n",
    "num_layers = len(list(symbolic_circuit.layers))\n",
    "print(f\"Number of layers: {num_layers}\")\n",
    "num_parameters = sum(p.numel() for p in circuit.parameters())\n",
    "print(f\"Number of learnable parameters: {num_parameters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beee1f04",
   "metadata": {},
   "source": [
    "## Learning a Probabilistic Circuit using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bf4858",
   "metadata": {},
   "source": [
    "Learning the probabilistic circuit we have compiled above can be done _in the same way as any other neural network_ written using PyTorch.\n",
    "\n",
    "Next, we load MNIST with [torchvision](https://pytorch.org/vision/stable/index.html), and select one of the many optimizers available in PyTorch, such as [Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02854883",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T14:18:58.418158Z",
     "start_time": "2024-10-09T14:18:57.224536Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "# Load the MNIST data set and data loaders\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # Flatten the images and set pixel values in the [0-255] range\n",
    "    transforms.Lambda(lambda x: (255 * x.view(-1)).long())\n",
    "])\n",
    "data_train = datasets.MNIST('datasets', train=True, download=True, transform=transform)\n",
    "data_test = datasets.MNIST('datasets', train=False, download=True, transform=transform)\n",
    "\n",
    "# Instantiate the training and testing data loaders\n",
    "train_dataloader = DataLoader(data_train, shuffle=True, batch_size=256)\n",
    "test_dataloader = DataLoader(data_test, shuffle=False, batch_size=256)\n",
    "\n",
    "# Initialize a torch optimizer of your choice,\n",
    "#  e.g., Adam, by passing the parameters of the circuit\n",
    "optimizer = optim.Adam(circuit.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e08fe19-607d-42dc-8ec7-3ab1b6110274",
   "metadata": {},
   "source": [
    "Finally, we write a basic training loop to iterate over MNIST images for some epochs, optimizing the circuit parameters by minimizing the average  negative log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f28e9c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T14:21:45.586683Z",
     "start_time": "2024-10-09T14:18:58.426109Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500: Average NLL: 1517.053\n",
      "Step 1000: Average NLL: 748.464\n",
      "Step 1500: Average NLL: 709.410\n",
      "Step 2000: Average NLL: 691.506\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "step_idx = 0\n",
    "running_loss = 0.0\n",
    "running_samples = 0\n",
    "\n",
    "# Move the circuit to chosen device\n",
    "circuit = circuit.to(device)\n",
    "\n",
    "for epoch_idx in range(num_epochs):\n",
    "    for i, (batch, _) in enumerate(train_dataloader):\n",
    "        # The circuit expects an input of shape (batch_dim, num_variables)\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Compute the log-likelihoods of the batch, by evaluating the circuit\n",
    "        log_likelihoods = circuit(batch)\n",
    "\n",
    "        # We take the negated average log-likelihood as loss\n",
    "        loss = -torch.mean(log_likelihoods)\n",
    "        loss.backward()\n",
    "        # Update the parameters of the circuits, as any other model in PyTorch\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        running_loss += loss.detach() * len(batch)\n",
    "        running_samples += len(batch)\n",
    "        step_idx += 1\n",
    "        if step_idx % 500 == 0:\n",
    "            average_nll = running_loss / running_samples\n",
    "            print(f\"Step {step_idx}: Average NLL: {average_nll:.3f}\")\n",
    "            running_loss = 0.0\n",
    "            running_samples = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50efae6b",
   "metadata": {},
   "source": [
    "Then, we can evaluate our probabilistic circuit on test data by computing the average log-likelihood and bits per dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e66bd8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T14:21:48.557821Z",
     "start_time": "2024-10-09T14:21:45.715485Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test LL: -681.926\n",
      "Bits per dimension: 1.255\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    test_lls = 0.0\n",
    "\n",
    "    for batch, _ in test_dataloader:\n",
    "        # The circuit expects an input of shape (batch_dim, num_channels, num_variables),\n",
    "        # so we unsqueeze a dimension for the channel.\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Compute the log-likelihoods of the batch\n",
    "        log_likelihoods = circuit(batch)\n",
    "\n",
    "        # Accumulate the log-likelihoods\n",
    "        test_lls += log_likelihoods.sum().item()\n",
    "\n",
    "    # Compute average test log-likelihood and bits per dimension\n",
    "    average_ll = test_lls / len(data_test)\n",
    "    bpd = -average_ll / (28 * 28 * np.log(2.0))\n",
    "    print(f\"Average test LL: {average_ll:.3f}\")\n",
    "    print(f\"Bits per dimension: {bpd:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7ddd59-5a79-422f-bfb6-4ea85a426a33",
   "metadata": {},
   "source": [
    "This is not the end of the story, however. With a circuit we can do many more operations, such as sampling (as we did in [the GMM notebook](../learning-a-gaussian-mixture-model)), as well as conditional and marginalizing over a subset of the inputs, as we will see in the [generative vs. discriminative circuits](../generative-vs-discriminative-circuit) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84df7bd0-a160-452e-86c6-b97406d6fb75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
