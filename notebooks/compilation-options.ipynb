{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a61029f7-31c0-41d3-ad93-ed730b12646e",
   "metadata": {},
   "source": [
    "# Setting Options of the Compilation Backend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78aa8c28-cc8b-45bd-9f10-3dd356c74777",
   "metadata": {},
   "source": [
    "We explore the available options that can be specified when compiling a symbolic circuit. See the notebook [learning-a-circuit.ipynb](./learning-a-circuit.ipynb) for more details about symbolic circuit representations and their compilation. Currently, symbolic circuits can only be compiled using a PyTorch 2+ backend, which allows you to specify a few options, such as the semiring that defines how to evaluate sum and products and a couple of flags related to optimizations. Future versions of ```cirkit``` may include compilation backends other than PyTorch, each with their own set of features and compilation options. However, the philosophy of ```cirkit``` is to abstract away the design of circuits and their operators from the underlying implementation and deep learning library dependencies. This will foster opportunities arising from connecting different platforms and compiler tool chains, without affecting the rest of the library.\n",
    "\n",
    "We start by instantiating a symbolic circuit for image data, as shown in the following code. Note that this is completely disentangled from the compilation step and the compilation options we explore next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "295597da-8a26-4ec6-9ffc-4b30feacc6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirkit.templates import data_modalities, utils\n",
    "\n",
    "symbolic_circuit = data_modalities.image_data(\n",
    "    (1, 28, 28),                # The shape of the image, i.e., (num_channels, image_height, image_width)\n",
    "    region_graph='quad-graph',  # Select the structure of the circuit to follow the QuadGraph region graph\n",
    "    input_layer='categorical',  # Use Categorical distributions for the pixel values (0-255) as input layers\n",
    "    num_input_units=64,         # Each input layer consists of 64 Categorical input units\n",
    "    sum_product_layer='tucker', # Use Tucker sum-product layers, i.e., alternate dense sum layers and kronecker product layers\n",
    "    num_sum_units=64,           # Each dense sum layer consists of 64 sum units\n",
    "    sum_weight_param=utils.Parameterization(\n",
    "        activation='softmax',   # Parameterize the sum weights by using a softmax activation\n",
    "        initialization='normal' # Initialize the sum weights by sampling from a standard normal distribution\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284040e3-6b21-4cd6-b69b-c7135ebe0b40",
   "metadata": {},
   "source": [
    "## The Pipeline Context object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8026fd-3916-4064-960e-39654b07e491",
   "metadata": {},
   "source": [
    "The most important object we introduce in this notebook is the **pipeline context**, which allows you to specify the compilation backend, as well as compilation options. Since we will use the PyTorch backend, we first set some random seeds and the device to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "048243c9-1225-483e-a61c-ba3ca1bc4f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds and the torch device\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Set some seeds\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# Set the torch device to use\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94995ded-b1a7-4605-9fcf-c450aa0717de",
   "metadata": {},
   "source": [
    "In the next code snippet, we show how to instantiate a pipeline context using the PyTorch backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dae3949d-b342-4fe0-b185-d9435a9f3d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirkit.pipeline import PipelineContext\n",
    "\n",
    "ctx = PipelineContext(\n",
    "    backend='torch',  # Use the PyTorch backend with default compilation flags\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d35a14-a21b-4260-b16f-8314cad0f2b4",
   "metadata": {},
   "source": [
    "By using this pipeline context, we can compile symbolic circuits as shown in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d58e5ed2-0661-446d-bf36-795191e83b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "circuit = ctx.compile(symbolic_circuit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cd78de-37ee-4080-b35d-be112cd7eb3b",
   "metadata": {},
   "source": [
    "An alternative way to compile circuits using a pipeline context is by combining the ``with`` statement and the ``compile`` function, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01ef4a4d-745b-45c9-9272-ecdd56bc8d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirkit.pipeline import compile\n",
    "\n",
    "with ctx:\n",
    "    circuit = compile(symbolic_circuit)\n",
    "    # Many circuits can possibly be compiled using the same pipeline context\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b4155b-320f-4d60-8ae4-13f1211b7e1e",
   "metadata": {},
   "source": [
    "The PyTorch backend allows you to specify three compilation options: (1) a particular **semiring** that specifies how to evaluate sum and product layers, (2) **whether to fold** the circuit computational graph as to better exploit parallel architectures like GPUs or not, and (3) **whether to optimize** the layers and the parameters of each layer by enabling a number of optimization rules. Below, we discuss each of these compilation options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f901a9-fb2a-4a07-97c7-398b262c79d7",
   "metadata": {},
   "source": [
    "## (1) Choosing a Semiring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a343f09d-a6cd-4b03-ad2e-e57bf273e954",
   "metadata": {},
   "source": [
    "By default, the semiring used is the usual one defined over the reals (called ``sum-product``), i.e., the semiring $(\\mathbb{R},+,\\times)$, where $\\mathbb{R}$ is the field of real numbers, and $+$ and $\\times$ are the usual sum and products over reals. Another popular semiring is the _log-sum-exp and sum_ semiring (called ``lse-sum``), which ensures numerical stability by performing computations \"in log-space\". In fact, the ``lse-sum`` semiring is defined as $(\\mathbb{R},\\oplus,\\otimes)$, where $\\oplus$ is the log-sum-exp operation and $\\otimes$ is the sum. By specifying ``lse-sum`` as semiring, sums compute log-sum-exp operations, while products compute sums, hence avoiding numerical issues such as underflows. A third available semiring is the ``complex-lse-sum`` semiring, which extends the ``lse-sum`` semiring to the field of complex numbers $(\\mathbb{C},\\oplus,\\otimes)$, by making use of the complex extensions of logarithms and exponentials. This semiring is particularly useful to ensure numerical stability in the case of circuits with negative parameters.\n",
    "\n",
    "In the following code, we instantiate a pipeline context by specifying the ``lse-sum`` semiring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9e598b6-7ccb-4edc-9ff6-dd88d6c1af38",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = PipelineContext(\n",
    "    backend='torch',      # Use the PyTorch backend\n",
    "    # Specify the backend compilation flags next\n",
    "    # ---- Specify how to evaluate sum and product layers ---- #\n",
    "    semiring='lse-sum',   # In this case we use the numerically-stable 'lse-sum' semiring (R, +, *), i.e.,\n",
    "                          # where: + is the log-sum-exp operation, and * is the sum operation.\n",
    "    # -------------------------------------------------------- #\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e6f46d-9211-4dd2-b259-1651dff56832",
   "metadata": {},
   "source": [
    "Next, we compile the circuit using this pipeline context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a086946-4d55-4d70-867e-be2a60ca7fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.58 s, sys: 1.02 s, total: 5.6 s\n",
      "Wall time: 5.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "circuit = ctx.compile(symbolic_circuit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "605b6aa4-6d0a-4d64-9fee-03594381f7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "circuit.to(device);  # Move the compiled circuit parameters to the chosen device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d136e99-ecdf-47cf-8f26-aa189ceb8cfd",
   "metadata": {},
   "source": [
    "Since we have chosen the ``lse-sum`` semiring, we expect the compiled circuit to output log-probabilities rather than probabilities. We can quickly check this by evaluating the circuit on some input and observing that the outputs are negative (i.e., they are log-likelihoods)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fa834ad-3a76-4f29-a6a9-446191324ef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4358.77685546875"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = torch.randint(256, size=(1, 1, 784), device=device)\n",
    "circuit(batch).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bba636-3150-4d6c-bed0-b1505ea4abb0",
   "metadata": {},
   "source": [
    "In the next section of this notebook, we enable a couple of compilation flags that will speed up the feed-forward evaluation of a circuit. However, why would someone disable the optimizations in the first place? The answer is that disabling optimizations is great for debugging purposes. In fact, the PyTorch backend ensures a one-to-one correspondence between the layers in the symbolic circuit representation and the compiled layers, if no optimizations are enabled, thus simplifying debugging operations such as verifying the correctness of inputs and outputs of _each_ layer separately.\n",
    "\n",
    "Before proceeding to the next section, we benchmark the feed-forward evaluation of the circuit compiled with the default options, as it will serve as a reference when we will enable **folding** and **other optimizations**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e527a96-4659-44c2-9bea-b161963f1032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26 s ± 19.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "batch = torch.randint(256, size=(128, 1, 784), device=device)\n",
    "circuit(batch)\n",
    "if 'cuda' in str(device):\n",
    "    torch.cuda.synchronize(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c5a2bd-4517-4c91-ab09-375998e5095c",
   "metadata": {},
   "source": [
    "## (2) Folding your Circuit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec7fae1-68ae-4923-ada5-c1820f298201",
   "metadata": {},
   "source": [
    "Circuits typically have layers that can possibly be evaluated independently. Therefore, we can exploit powerful parallel architectures like GPUs to parallelize the computation of such layers. Enabling folding as compilation option _fuses_ layers of the same type (e.g., Kronecker product layers) that can be evaluated in parallel. By doing so, we obtain a much more efficient computational graph in PyTorch, with a negligible overhead in terms of compilation speed.\n",
    "\n",
    "To initialize a pipeline context that enables folding, we simply need to specify ``fold=True``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e42a5023-7147-41d9-919e-72e0552b1a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = PipelineContext(\n",
    "    backend='torch',      # Use the PyTorch backend\n",
    "    # Specify the backend compilation flags next\n",
    "    semiring='lse-sum',   # Use the 'lse-sum' semiring\n",
    "    # --------- Enable circuit folding ---------- #\n",
    "    fold=True,\n",
    "    # ------------------------------------------- #\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60950b0b-9cb0-4073-965c-6f7db7ad2d6f",
   "metadata": {},
   "source": [
    "Next, we compile the same symbolic circuit and obtain a folded circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f737ee9b-dc7a-437e-877d-74c876df3c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.6 s, sys: 989 ms, total: 5.59 s\n",
      "Wall time: 5.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "folded_circuit = ctx.compile(symbolic_circuit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8c3e088-b49d-460d-8ac7-74ae7ccdf16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "folded_circuit.to(device);  # Move the compiled circuit parameters to the chosen device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16615fd-38e8-4b7f-b03b-8e30502ce965",
   "metadata": {},
   "source": [
    "Note that the compilation procedure took a similar amount of time, when compared to the compilation with the default compilation options shown above. In addition, we compare the number of layers of an \"unfolded\" circuits with the number of layers of a \"folded\" circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d15f7585-8ea5-4a0b-a22d-9025d30cb3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers (fold=False): 4163\n",
      "Number of layers (fold=True):  26\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of layers (fold=False): {len(circuit.layers)}')\n",
    "print(f'Number of layers (fold=True):  {len(folded_circuit.layers)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fe5fa1-04fe-49b8-886d-13bfadf68af6",
   "metadata": {},
   "source": [
    "The \"folded\" circuit has far fewer layers, since many of them have been fused together. For example, we can check that the first layer of the circuit computing Categorical likelihoods consists of many folds, as many as the number of variables modelling MNIST images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb933237-a1a6-4ca5-a6b9-35fef9afddc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of the input folded layer: TorchCategoricalLayer\n",
      "Number of folded layers within it: 784\n"
     ]
    }
   ],
   "source": [
    "folded_layer = next(folded_circuit.topological_ordering())\n",
    "print(f'Type of the input folded layer: {folded_layer.__class__.__name__}')\n",
    "print(f'Number of folded layers within it: {folded_layer.num_folds}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f074e168-dee4-4234-8eae-afd28fae317f",
   "metadata": {},
   "source": [
    "As we see in the next code snippet, enabling folding provided an (approximately) **19.9x speed-up** for feed-forward circuit evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad419f94-c4d6-4051-a817-5bb8b85382ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58.9 ms ± 22.1 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "batch = torch.randint(256, size=(128, 1, 784), device=device)\n",
    "folded_circuit(batch)\n",
    "if 'cuda' in str(device):\n",
    "    torch.cuda.synchronize(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13286bb-afa1-4284-8087-04fba7d65289",
   "metadata": {},
   "source": [
    "## (3) Optimizing the Circuit Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e8d4b6-53e7-4467-8469-02c15de93f70",
   "metadata": {},
   "source": [
    "Some circuits have layers and parameterizations whose evaluation can be optimized. Enabling optimizations in a pipeline context tells the compiler to try matching a number of optimization patterns defined over the layers of the circuit. If an optimization pattern matches, then the compiler performs a number of operations to optimize the circuit structure.\n",
    "\n",
    "A simple example of an optimizable circuit structure is the one that alternates Kronecker product layers with Dense sum layers. The symbolic circuit we have built has already this kind of circuit structure, as we have specified the ``tucker`` sum-product layer. We can verify this by observing the types of the layers of the folded circuit have compiled above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91122716-845b-488a-a747-1d0972c181fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TorchCategoricalLayer', 'TorchKroneckerLayer', 'TorchSumLayer', 'TorchKroneckerLayer', 'TorchSumLayer', 'TorchSumLayer', 'TorchKroneckerLayer', 'TorchSumLayer', 'TorchKroneckerLayer', 'TorchSumLayer', 'TorchSumLayer', 'TorchKroneckerLayer', 'TorchSumLayer', 'TorchKroneckerLayer', 'TorchSumLayer', 'TorchSumLayer', 'TorchKroneckerLayer', 'TorchSumLayer', 'TorchKroneckerLayer', 'TorchSumLayer', 'TorchSumLayer', 'TorchKroneckerLayer', 'TorchSumLayer', 'TorchKroneckerLayer', 'TorchSumLayer', 'TorchSumLayer']\n"
     ]
    }
   ],
   "source": [
    "print([layer.__class__.__name__ for layer in folded_circuit.layers])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3873403-7c97-4932-aeda-12d45e9b679e",
   "metadata": {},
   "source": [
    "In this case, we can fuse Kronecker and Dense layers in a single layer, which we call Tucker layer, that performs the same computations using an efficient ``einsum`` tensorized operation. This optimization is why probabilistic circuit architectures like [EinsumNetworks](https://arxiv.org/abs/2004.06231) are much more efficient. However, there are many other compilation rules that are currently supported by the PyTorch backend.\n",
    "\n",
    "The next piece of code shows how to enable optimizations in a pipeline context (i.e., specify ``optimize=True``)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aed662c8-79b5-4e05-8564-6e65b47a0478",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = PipelineContext(\n",
    "    backend='torch',      # Use the PyTorch backend\n",
    "    # Specify the backend compilation flags next\n",
    "    semiring='lse-sum',   # Use the 'lse-sum' semiring\n",
    "    fold=True,            # Enable circuit folding\n",
    "    # -------- Enable layer optimizations -------- #\n",
    "    optimize=True,\n",
    "    # -------------------------------------------- #\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c85a85e-2dd2-4527-8c44-58943c397111",
   "metadata": {},
   "source": [
    "Next, we compile the same symbolic circuit and obtain an optimized circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05afb743-3489-45ff-9e1e-9460828ee808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.97 s, sys: 962 ms, total: 5.93 s\n",
      "Wall time: 5.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "optimized_circuit = ctx.compile(symbolic_circuit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b87a7f78-0c8f-4c47-a3a2-027a39f60eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_circuit.to(device);  # Move the compiled circuit parameters to the chosen device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e482ab-56db-41f5-9c34-c4d28eba6ac9",
   "metadata": {},
   "source": [
    "Note that the compilation took just a little more time than the time for the folded circuit. Moreover, if we look at the list of layers, we observe that some of them are now Tucker layers, which can be much more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0c1fcf6-9f37-411e-ac3e-ddc8c5376194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TorchCategoricalLayer', 'TorchTuckerLayer', 'TorchTuckerLayer', 'TorchSumLayer', 'TorchTuckerLayer', 'TorchTuckerLayer', 'TorchSumLayer', 'TorchTuckerLayer', 'TorchTuckerLayer', 'TorchSumLayer', 'TorchTuckerLayer', 'TorchTuckerLayer', 'TorchSumLayer', 'TorchTuckerLayer', 'TorchTuckerLayer', 'TorchSumLayer']\n"
     ]
    }
   ],
   "source": [
    "print([layer.__class__.__name__ for layer in optimized_circuit.layers])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94aa778-d56c-44ac-8556-be0ce247335a",
   "metadata": {},
   "source": [
    "Finally, we benchmark the optimized circuit compiled in this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58dbcb48-e26a-4df3-87d0-7be6daa41ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.6 ms ± 4.2 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "batch = torch.randint(256, size=(128, 1, 784), device=device)\n",
    "optimized_circuit(batch)\n",
    "if 'cuda' in str(device):\n",
    "    torch.cuda.synchronize(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d95c02-2c66-4414-b676-0dec303f2aa9",
   "metadata": {},
   "source": [
    "Note that, we achieved an (approximately) **2.3x speed-up**, when compared to the folded circuit compiled above, and an (approximately) **45.7x speed-up**, when compared to the circuit compiled with no folding and no optimizations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
